# Comparing `tmp/asyncdb-2.7.3-pp39-pypy39_pp73-win_amd64.whl.zip` & `tmp/asyncdb-2.7.4-pp39-pypy39_pp73-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,71 +1,71 @@
-Zip file size: 216339 bytes, number of entries: 69
--rw-rw-rw-  2.0 fat      411 b- defN 24-May-13 18:56 asyncdb/__init__.py
--rw-rw-rw-  2.0 fat     1297 b- defN 24-May-13 18:56 asyncdb/connections.py
--rw-rw-rw-  2.0 fat    25523 b- defN 24-May-13 18:56 asyncdb/interfaces.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-13 18:56 asyncdb/py.typed
--rw-rw-rw-  2.0 fat      280 b- defN 24-May-13 18:56 asyncdb/version.py
--rw-rw-rw-  2.0 fat       28 b- defN 24-May-13 18:56 asyncdb/drivers/__init__.py
--rw-rw-rw-  2.0 fat     4529 b- defN 24-May-13 18:56 asyncdb/drivers/abstract.py
--rw-rw-rw-  2.0 fat    17819 b- defN 24-May-13 18:56 asyncdb/drivers/bigquery.py
--rw-rw-rw-  2.0 fat    17209 b- defN 24-May-13 18:56 asyncdb/drivers/cassandra.py
--rw-rw-rw-  2.0 fat    10172 b- defN 24-May-13 18:56 asyncdb/drivers/delta.py
--rw-rw-rw-  2.0 fat    10193 b- defN 24-May-13 18:56 asyncdb/drivers/duckdb.py
--rw-rw-rw-  2.0 fat     2110 b- defN 24-May-13 18:56 asyncdb/drivers/dummy.py
--rw-rw-rw-  2.0 fat    13068 b- defN 24-May-13 18:56 asyncdb/drivers/hazel.py
--rw-rw-rw-  2.0 fat    20827 b- defN 24-May-13 18:56 asyncdb/drivers/influx.py
--rw-rw-rw-  2.0 fat    29764 b- defN 24-May-13 18:56 asyncdb/drivers/jdbc.py
--rw-rw-rw-  2.0 fat     6638 b- defN 24-May-13 18:56 asyncdb/drivers/mcache.py
--rw-rw-rw-  2.0 fat     9336 b- defN 24-May-13 18:56 asyncdb/drivers/memcache.py
--rw-rw-rw-  2.0 fat     3316 b- defN 24-May-13 18:56 asyncdb/drivers/mongo.py
--rw-rw-rw-  2.0 fat    19474 b- defN 24-May-13 18:56 asyncdb/drivers/mredis.py
--rw-rw-rw-  2.0 fat    15824 b- defN 24-May-13 18:56 asyncdb/drivers/mssql.py
--rw-rw-rw-  2.0 fat    16072 b- defN 24-May-13 18:56 asyncdb/drivers/mysql.py
--rw-rw-rw-  2.0 fat    18635 b- defN 24-May-13 18:56 asyncdb/drivers/mysqlclient.py
--rw-rw-rw-  2.0 fat     8036 b- defN 24-May-13 18:56 asyncdb/drivers/odbc.py
--rw-rw-rw-  2.0 fat     4206 b- defN 24-May-13 18:56 asyncdb/drivers/oracle.py
--rw-rw-rw-  2.0 fat    61614 b- defN 24-May-13 18:56 asyncdb/drivers/pg.py
--rw-rw-rw-  2.0 fat    22590 b- defN 24-May-13 18:56 asyncdb/drivers/postgres.py
--rw-rw-rw-  2.0 fat    19814 b- defN 24-May-13 18:56 asyncdb/drivers/redis.py
--rw-rw-rw-  2.0 fat    35185 b- defN 24-May-13 18:56 asyncdb/drivers/rethink.py
--rw-rw-rw-  2.0 fat    16815 b- defN 24-May-13 18:56 asyncdb/drivers/sa.py
--rw-rw-rw-  2.0 fat    56513 b- defN 24-May-13 18:56 asyncdb/drivers/scylladb.py
--rw-rw-rw-  2.0 fat     2071 b- defN 24-May-13 18:56 asyncdb/drivers/sql.py
--rw-rw-rw-  2.0 fat    25972 b- defN 24-May-13 18:56 asyncdb/drivers/sqlite.py
--rw-rw-rw-  2.0 fat    14374 b- defN 24-May-13 18:56 asyncdb/drivers/sqlserver.py
--rw-rw-rw-  2.0 fat      513 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/__init__.py
--rw-rw-rw-  2.0 fat      848 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/arrow.py
--rw-rw-rw-  2.0 fat      429 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/base.py
--rw-rw-rw-  2.0 fat      997 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/csv.py
--rw-rw-rw-  2.0 fat      693 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/dataclass.py
--rw-rw-rw-  2.0 fat      827 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/dt.py
--rw-rw-rw-  2.0 fat      408 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/generator.py
--rw-rw-rw-  2.0 fat      543 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/iter.py
--rw-rw-rw-  2.0 fat      418 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/json.py
--rw-rw-rw-  2.0 fat      986 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/output.py
--rw-rw-rw-  2.0 fat     1021 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/pandas.py
--rw-rw-rw-  2.0 fat      864 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/polars.py
--rw-rw-rw-  2.0 fat     1236 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/pyspark.py
--rw-rw-rw-  2.0 fat     1038 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/record.py
--rw-rw-rw-  2.0 fat      770 b- defN 24-May-13 18:56 asyncdb/drivers/outputs/recordset.py
--rw-rw-rw-  2.0 fat      867 b- defN 24-May-13 18:56 asyncdb/exceptions/__init__.py
--rw-rw-rw-  2.0 fat   158720 b- defN 24-May-13 19:03 asyncdb/exceptions/exceptions.pypy39-pp73-win_amd64.pyd
--rw-rw-rw-  2.0 fat     2539 b- defN 24-May-13 18:56 asyncdb/exceptions/handlers.py
--rw-rw-rw-  2.0 fat      176 b- defN 24-May-13 18:56 asyncdb/meta/__init__.py
--rw-rw-rw-  2.0 fat     3057 b- defN 24-May-13 18:56 asyncdb/meta/record.py
--rw-rw-rw-  2.0 fat     2221 b- defN 24-May-13 18:56 asyncdb/meta/recordset.py
--rw-rw-rw-  2.0 fat      542 b- defN 24-May-13 18:56 asyncdb/models/__init__.py
--rw-rw-rw-  2.0 fat    17784 b- defN 24-May-13 18:56 asyncdb/models/model.py
--rw-rw-rw-  2.0 fat      136 b- defN 24-May-13 18:56 asyncdb/utils/__init__.py
--rw-rw-rw-  2.0 fat     2484 b- defN 24-May-13 18:56 asyncdb/utils/functions.py
--rw-rw-rw-  2.0 fat      655 b- defN 24-May-13 18:56 asyncdb/utils/modules.py
--rw-rw-rw-  2.0 fat   111616 b- defN 24-May-13 19:03 asyncdb/utils/types.pypy39-pp73-win_amd64.pyd
--rw-rw-rw-  2.0 fat      319 b- defN 24-May-13 18:56 asyncdb/utils/uv.py
--rw-rw-rw-  2.0 fat      234 b- defN 24-May-13 18:56 asyncdb/utils/encoders/__init__.py
--rw-rw-rw-  2.0 fat      487 b- defN 24-May-13 18:56 asyncdb/utils/encoders/numpy.py
--rw-rw-rw-  2.0 fat     1195 b- defN 24-May-13 18:56 asyncdb/utils/encoders/pg.py
--rw-rw-rw-  2.0 fat     1538 b- defN 24-May-13 19:03 asyncdb-2.7.3.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    12805 b- defN 24-May-13 19:03 asyncdb-2.7.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat      107 b- defN 24-May-13 19:03 asyncdb-2.7.3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 24-May-13 19:03 asyncdb-2.7.3.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     5791 b- defN 24-May-13 19:03 asyncdb-2.7.3.dist-info/RECORD
-69 files, 844587 bytes uncompressed, 207239 bytes compressed:  75.5%
+Zip file size: 217442 bytes, number of entries: 69
+-rw-rw-rw-  2.0 fat      411 b- defN 24-May-14 00:32 asyncdb/__init__.py
+-rw-rw-rw-  2.0 fat     1329 b- defN 24-May-14 00:32 asyncdb/connections.py
+-rw-rw-rw-  2.0 fat    25523 b- defN 24-May-14 00:32 asyncdb/interfaces.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-14 00:32 asyncdb/py.typed
+-rw-rw-rw-  2.0 fat      280 b- defN 24-May-14 00:32 asyncdb/version.py
+-rw-rw-rw-  2.0 fat       28 b- defN 24-May-14 00:32 asyncdb/drivers/__init__.py
+-rw-rw-rw-  2.0 fat     4529 b- defN 24-May-14 00:32 asyncdb/drivers/abstract.py
+-rw-rw-rw-  2.0 fat    19346 b- defN 24-May-14 00:32 asyncdb/drivers/bigquery.py
+-rw-rw-rw-  2.0 fat    17209 b- defN 24-May-14 00:32 asyncdb/drivers/cassandra.py
+-rw-rw-rw-  2.0 fat    14737 b- defN 24-May-14 00:32 asyncdb/drivers/delta.py
+-rw-rw-rw-  2.0 fat    10233 b- defN 24-May-14 00:32 asyncdb/drivers/duckdb.py
+-rw-rw-rw-  2.0 fat     2110 b- defN 24-May-14 00:32 asyncdb/drivers/dummy.py
+-rw-rw-rw-  2.0 fat    13068 b- defN 24-May-14 00:32 asyncdb/drivers/hazel.py
+-rw-rw-rw-  2.0 fat    20827 b- defN 24-May-14 00:32 asyncdb/drivers/influx.py
+-rw-rw-rw-  2.0 fat    29764 b- defN 24-May-14 00:32 asyncdb/drivers/jdbc.py
+-rw-rw-rw-  2.0 fat     6638 b- defN 24-May-14 00:32 asyncdb/drivers/mcache.py
+-rw-rw-rw-  2.0 fat     9336 b- defN 24-May-14 00:32 asyncdb/drivers/memcache.py
+-rw-rw-rw-  2.0 fat     3316 b- defN 24-May-14 00:32 asyncdb/drivers/mongo.py
+-rw-rw-rw-  2.0 fat    19474 b- defN 24-May-14 00:32 asyncdb/drivers/mredis.py
+-rw-rw-rw-  2.0 fat    15824 b- defN 24-May-14 00:32 asyncdb/drivers/mssql.py
+-rw-rw-rw-  2.0 fat    16072 b- defN 24-May-14 00:32 asyncdb/drivers/mysql.py
+-rw-rw-rw-  2.0 fat    18635 b- defN 24-May-14 00:32 asyncdb/drivers/mysqlclient.py
+-rw-rw-rw-  2.0 fat     8036 b- defN 24-May-14 00:32 asyncdb/drivers/odbc.py
+-rw-rw-rw-  2.0 fat     4206 b- defN 24-May-14 00:32 asyncdb/drivers/oracle.py
+-rw-rw-rw-  2.0 fat    61614 b- defN 24-May-14 00:32 asyncdb/drivers/pg.py
+-rw-rw-rw-  2.0 fat    22590 b- defN 24-May-14 00:32 asyncdb/drivers/postgres.py
+-rw-rw-rw-  2.0 fat    19814 b- defN 24-May-14 00:32 asyncdb/drivers/redis.py
+-rw-rw-rw-  2.0 fat    35185 b- defN 24-May-14 00:32 asyncdb/drivers/rethink.py
+-rw-rw-rw-  2.0 fat    16815 b- defN 24-May-14 00:32 asyncdb/drivers/sa.py
+-rw-rw-rw-  2.0 fat    56513 b- defN 24-May-14 00:32 asyncdb/drivers/scylladb.py
+-rw-rw-rw-  2.0 fat     2071 b- defN 24-May-14 00:32 asyncdb/drivers/sql.py
+-rw-rw-rw-  2.0 fat    25972 b- defN 24-May-14 00:32 asyncdb/drivers/sqlite.py
+-rw-rw-rw-  2.0 fat    14374 b- defN 24-May-14 00:32 asyncdb/drivers/sqlserver.py
+-rw-rw-rw-  2.0 fat      513 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/__init__.py
+-rw-rw-rw-  2.0 fat      848 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/arrow.py
+-rw-rw-rw-  2.0 fat      429 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/base.py
+-rw-rw-rw-  2.0 fat      997 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/csv.py
+-rw-rw-rw-  2.0 fat      693 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/dataclass.py
+-rw-rw-rw-  2.0 fat      827 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/dt.py
+-rw-rw-rw-  2.0 fat      408 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/generator.py
+-rw-rw-rw-  2.0 fat      543 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/iter.py
+-rw-rw-rw-  2.0 fat      418 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/json.py
+-rw-rw-rw-  2.0 fat      986 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/output.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/pandas.py
+-rw-rw-rw-  2.0 fat      864 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/polars.py
+-rw-rw-rw-  2.0 fat     1236 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/pyspark.py
+-rw-rw-rw-  2.0 fat     1038 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/record.py
+-rw-rw-rw-  2.0 fat      770 b- defN 24-May-14 00:32 asyncdb/drivers/outputs/recordset.py
+-rw-rw-rw-  2.0 fat      867 b- defN 24-May-14 00:32 asyncdb/exceptions/__init__.py
+-rw-rw-rw-  2.0 fat   158720 b- defN 24-May-14 00:39 asyncdb/exceptions/exceptions.pypy39-pp73-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     2539 b- defN 24-May-14 00:32 asyncdb/exceptions/handlers.py
+-rw-rw-rw-  2.0 fat      176 b- defN 24-May-14 00:32 asyncdb/meta/__init__.py
+-rw-rw-rw-  2.0 fat     3057 b- defN 24-May-14 00:32 asyncdb/meta/record.py
+-rw-rw-rw-  2.0 fat     2221 b- defN 24-May-14 00:32 asyncdb/meta/recordset.py
+-rw-rw-rw-  2.0 fat      542 b- defN 24-May-14 00:32 asyncdb/models/__init__.py
+-rw-rw-rw-  2.0 fat    17784 b- defN 24-May-14 00:32 asyncdb/models/model.py
+-rw-rw-rw-  2.0 fat      136 b- defN 24-May-14 00:32 asyncdb/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2484 b- defN 24-May-14 00:32 asyncdb/utils/functions.py
+-rw-rw-rw-  2.0 fat      655 b- defN 24-May-14 00:32 asyncdb/utils/modules.py
+-rw-rw-rw-  2.0 fat   111616 b- defN 24-May-14 00:39 asyncdb/utils/types.pypy39-pp73-win_amd64.pyd
+-rw-rw-rw-  2.0 fat      319 b- defN 24-May-14 00:32 asyncdb/utils/uv.py
+-rw-rw-rw-  2.0 fat      234 b- defN 24-May-14 00:32 asyncdb/utils/encoders/__init__.py
+-rw-rw-rw-  2.0 fat      487 b- defN 24-May-14 00:32 asyncdb/utils/encoders/numpy.py
+-rw-rw-rw-  2.0 fat     1195 b- defN 24-May-14 00:32 asyncdb/utils/encoders/pg.py
+-rw-rw-rw-  2.0 fat     1538 b- defN 24-May-14 00:39 asyncdb-2.7.4.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    12860 b- defN 24-May-14 00:39 asyncdb-2.7.4.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      107 b- defN 24-May-14 00:39 asyncdb-2.7.4.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 24-May-14 00:39 asyncdb-2.7.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     5791 b- defN 24-May-14 00:39 asyncdb-2.7.4.dist-info/RECORD
+69 files, 850806 bytes uncompressed, 208342 bytes compressed:  75.5%
```

## zipnote {}

```diff
@@ -186,23 +186,23 @@
 
 Filename: asyncdb/utils/encoders/numpy.py
 Comment: 
 
 Filename: asyncdb/utils/encoders/pg.py
 Comment: 
 
-Filename: asyncdb-2.7.3.dist-info/LICENSE
+Filename: asyncdb-2.7.4.dist-info/LICENSE
 Comment: 
 
-Filename: asyncdb-2.7.3.dist-info/METADATA
+Filename: asyncdb-2.7.4.dist-info/METADATA
 Comment: 
 
-Filename: asyncdb-2.7.3.dist-info/WHEEL
+Filename: asyncdb-2.7.4.dist-info/WHEEL
 Comment: 
 
-Filename: asyncdb-2.7.3.dist-info/top_level.txt
+Filename: asyncdb-2.7.4.dist-info/top_level.txt
 Comment: 
 
-Filename: asyncdb-2.7.3.dist-info/RECORD
+Filename: asyncdb-2.7.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## asyncdb/connections.py

```diff
@@ -37,8 +37,10 @@
         classpath = f"asyncdb.drivers.{driver}"
         try:
             mdl = module_exists(driver, classpath)
             obj = mdl(**kwargs)
             return obj
         except Exception as err:
             logging.exception(err)
-            raise DriverError(message=f"Cannot Load Backend {driver}") from err
+            raise DriverError(
+                message=f"Cannot Load Backend {driver}"
+            ) from err
```

## asyncdb/version.py

```diff
@@ -1,9 +1,9 @@
 """AsyncDB Meta information."""
 
 __title__ = "asyncdb"
 __description__ = "Library for Asynchronous data source connections \
     Collection of asyncio drivers."
-__version__ = "2.7.3"
+__version__ = "2.7.4"
 __author__ = "Jesus Lara"
 __author_email__ = "jesuslarag@gmail.com"
 __license__ = "BSD"
```

## asyncdb/drivers/bigquery.py

```diff
@@ -3,21 +3,15 @@
 from collections.abc import Iterable
 import io
 from pathlib import Path, PurePath
 import asyncio
 import aiofiles
 import pandas_gbq
 import pandas as pd
-try:
-    from google.cloud import storage
-except ImportError:
-    raise ImportError(
-        "BigQuery: google-cloud-storage library not found. Hint: Please install it using 'pip install google-cloud-storage'"
-    )
-
+from google.cloud import storage
 from google.cloud import bigquery as bq
 from google.cloud.exceptions import Conflict
 from google.cloud.bigquery import LoadJobConfig, SourceFormat
 from google.oauth2 import service_account
 from ..exceptions import DriverError
 from .sql import SQLDriver
 
@@ -34,31 +28,39 @@
         self._account = None
         self._dsn = ""
         self._project_id = params.get("project_id", None)
         super().__init__(dsn=dsn, loop=loop, params=params, **kwargs)
         if not self._credentials:
             self._account = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", None)
         if self._account is None and self._credentials is None:
-            raise DriverError("BigQuery: Missing account Credentials")
+            raise DriverError(
+                "BigQuery: Missing account Credentials"
+            )
         self._connection = None  # BigQuery does not use traditional connections
 
     async def connection(self):
         """Initialize BigQuery client.
         # Assuming that authentication is handled outside (via environment variables or similar)
         """
         try:
             if self._credentials:  # usage of explicit credentials
-                self.credentials = service_account.Credentials.from_service_account_file(self._credentials)
+                self.credentials = service_account.Credentials.from_service_account_file(
+                    self._credentials
+                )
                 if not self._project_id:
                     self._project_id = self.credentials.project_id
-                self._connection = bq.Client(credentials=self.credentials, project=self._project_id)
+                self._connection = bq.Client(
+                    credentials=self.credentials,
+                    project=self._project_id
+                )
                 self._connected = True
             else:
                 self.credentials = self._account
                 self._connection = bq.Client(project=self._project_id)
+            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(self._credentials)
         except Exception as e:
             raise DriverError(
                 f"BigQuery: Error initializing client: {e}"
             )
         return self
 
     async def close(self):
@@ -254,39 +256,49 @@
 
     async def write(
         self,
         table_id: str,
         data,
         dataset_id: str = None,
         use_streams: bool = False,
-        use_pandas: bool = False,
+        use_pandas: bool = True,  # by default using BigQuery
         if_exists: str = "append",
         **kwargs,
     ):
         """
         Write data to a BigQuery table
         """
         if not self._connection:
             await self.connection()
-        table = f"{self._connection.project}.{dataset_id}.{table_id}"
+        job = None
+        table = f"{self._project_id}.{dataset_id}.{table_id}"
         try:
             if isinstance(data, pd.DataFrame):
                 if use_pandas is True:
                     job = await self._thread_func(
                         self._connection.load_table_from_dataframe,
                         data,
                         table,
                         **kwargs
                     )
                 else:
+                    object_cols = data.select_dtypes(include=['object']).columns
+                    for column in object_cols:
+                        dtype = str(type(data[column].values[0]))
+                        if dtype == "<class 'datetime.date'>":
+                            data[column]  = pd.to_datetime(
+                                data[column],
+                                infer_datetime_format=True
+                            )
                     table = f"{dataset_id}.{table_id}"
                     job = await self._thread_func(
                         data.to_gbq,
                         table,
                         project_id=self._project_id,
+                        credentials=self.credentials,
                         if_exists=if_exists
                     )
             elif isinstance(data, list):
                 dataset_ref = self._connection.dataset(dataset_id)
                 table_ref = dataset_ref.table(table_id)
                 table = bq.Table(table_ref)
                 if use_streams is True:
@@ -309,18 +321,19 @@
                     )
                     loop = asyncio.get_event_loop()
                     await loop.run_in_executor(None, job.result)
                     if job.errors and len(job.errors) > 0:
                         raise RuntimeError(f"Job failed with errors: {job.errors}")
                     else:
                         self._logger.info(f"Loaded {len(data)} rows into {table_id}")
-
             self._logger.info(
                 f"Inserted rows into {dataset_id}.{table_id}"
             )
+            # return Job object
+            return job
         except Exception as e:
             raise DriverError(
                 f"BigQuery: Error writing to table: {e}"
             )
 
     async def load_table_from_uri(
         self,
@@ -373,57 +386,77 @@
         raise NotImplementedError  # pragma: no cover
 
     async def create_gcs_from_csv(
         self,
         bucket_name: str,
         object_name: str,
         csv_data: Union[bytes, PurePath, pd.DataFrame],
+        overwrite: bool = False,
         **kwargs
-    ):
+    ) -> tuple:
         """Creates a GCS object from CSV data."""
+        # we cannot import directly at the top level
+        credentials = service_account.Credentials.from_service_account_file(
+            self._credentials
+        )
         if isinstance(csv_data, PurePath) and csv_data.is_file():
             async with aiofiles.open(csv_data, mode="rb") as file:
                 csv_data = await file.read()
         elif isinstance(csv_data, pd.DataFrame):
             csv_data = csv_data.to_csv(index=False)
         elif not isinstance(csv_data, bytes):
             raise DriverError("BigQuery: Invalid file object")
         try:
-            storage_client = storage.Client()
+            storage_client = storage.Client(
+                credentials=credentials,
+                project=credentials.project_id
+            )
             bucket = storage_client.bucket(bucket_name)
             blob = bucket.blob(object_name)
-            # Option 1: Upload from a string
+            if blob.exists():
+                if not overwrite:
+                    return f"gs://{bucket_name}/{object_name}", "Object already exists and overwrite is set to False."
+                else:
+                    self._logger.info(
+                        f"Object {object_name} exists in {bucket_name} and will be overwritten."
+                    )
+            # Upload from a string
             blob.upload_from_string(csv_data, content_type='text/csv')
             # If successful, return the GCS URI
             gcs_uri = f"gs://{bucket_name}/{object_name}"
-            return gcs_uri
+            return gcs_uri, None
         except Exception as e:
             raise DriverError(
                 f"BigQuery: Error creating GCS object: {e}"
             )
 
     async def read_csv_from_gcs(
         self,
         table_id: str,
         dataset_id: str,
-        bucket_name: str,
-        object_name: str,
+        bucket_uri: str = None,
+        bucket_name: str = None,
+        object_name: str = None,
         **kwargs
     ):
         """Load data into a BigQuery table from a CSV file in GCS."""
         try:
-            gcs_uri = f"gs://{bucket_name}/{object_name}"
+            if not bucket_uri:
+                gcs_uri = f"gs://{bucket_name}/{object_name}"
+            else:
+                gcs_uri = bucket_uri
             job_config = LoadJobConfig(
                 source_format=SourceFormat.CSV,
                 autodetect=True,
                 **kwargs
             )
+            table = f"{self._project_id}.{dataset_id}.{table_id}"
             job = self._connection.load_table_from_uri(
                 gcs_uri,
-                table_id,
+                table,
                 job_config=job_config
             )
             job.result()  # Wait for the job to complete
             return job
         except Exception as e:
             raise DriverError(
                 f"BigQuery: Error loading from CSV in GCS: {e}"
```

## asyncdb/drivers/delta.py

```diff
@@ -3,93 +3,115 @@
 Notes on memcache Provider
 --------------------
 This provider implements a simple subset of funcionalities
 over DeltaLake DeltaTable Protocol.
 TODO: add Thread Pool Support.
 """
 import asyncio
+from collections.abc import Iterable
 import time
+import duckdb
 from typing import Any, Union, Optional
 from datetime import datetime
-from pathlib import Path
+from pathlib import Path, PurePath
+import polars as pl
 import pyarrow.parquet as pq
 import pyarrow.csv as pcsv
+import pyarrow.dataset as ds
 from pyarrow import fs
 import pandas as pd
 import datatable as dt
-from deltalake import DeltaTable
-from deltalake import PyDeltaTableError
-from deltalake.table import DeltaTableProtocolError
-from deltalake.writer import write_deltalake
+from deltalake import DeltaTable, write_deltalake
+from deltalake.exceptions import DeltaError, DeltaProtocolError
 from ..exceptions import DriverError
 from .abstract import (
     InitDriver,
 )
 
 
 class delta(InitDriver):
     _provider = "delta"
     _syntax = "nosql"
 
-    def __init__(self, loop: asyncio.AbstractEventLoop = None, params: dict = None, **kwargs) -> None:
-        try:
-            self.storage_options = params["storage_options"]
-            del params["storage_options"]
-        except KeyError:
-            self.storage_options = {}
-        try:
-            self.filename = params["filename"]
-            del params["filename"]
-        except KeyError as ex:
-            raise DriverError("Delta: Missing Filename on Parameters") from ex
+    def __init__(
+        self,
+        loop: asyncio.AbstractEventLoop = None,
+        params: dict = None,
+        **kwargs
+    ) -> None:
+
+        self.storage_options = params.pop("storage_options", {})
+        self._delta = params.pop('path', None)
         super().__init__(loop=loop, params=params, **kwargs)
         self.kwargs = params
 
     ### Context magic Methods
     def __enter__(self):
         return self
 
     def __exit__(self, *args):
         self.close()
 
-    # Create a memcache Connection
-    async def connection(self, version: int = None):  # pylint: disable=W0236
+    async def connection(
+        self,
+        path: Union[str, Path] = None,
+        version: int = None
+    ):  # pylint: disable=W0236
         """
-        __init Memcache initialization.
+        __init DeltaLake initialization.
         """
-        self._logger.info(f"DeltaTable: Connecting to {self.filename}")
+        if path:
+            self._delta = path
+        if not self._delta:
+            raise DriverError(
+                "Missing Path to DeltaTable."
+            )
+        self._logger.info(
+            f"DeltaTable: Connecting to {self._delta}"
+        )
         try:
             if version is not None:
                 self.kwargs["version"] = version
-            if self.filename.startswith("s3:"):
-                raw_fs, normalized_path = fs.FileSystem.from_uri(self.filename)
+            if self._delta.startswith("s3:"):
+                raw_fs, normalized_path = fs.FileSystem.from_uri(self._delta)
                 filesystem = fs.SubTreeFileSystem(normalized_path, raw_fs)
-                self._connection = DeltaTable(self.filename)
+                self._connection = DeltaTable(self._delta)
                 self._storage = self._connection.to_pyarrow_dataset(filesystem=filesystem)
-            # filesystem = fs.SubTreeFileSystem(self.filename, fs.LocalFileSystem())
             else:
-                self._connection = DeltaTable(self.filename, storage_options=self.storage_options, **self.kwargs)
-        except PyDeltaTableError as exc:
-            raise DriverError(message=f"{exc}") from exc
+                self._connection = DeltaTable(
+                    self._delta,
+                    storage_options=self.storage_options,
+                    **self.kwargs
+                )
+        except DeltaError as exc:
+            raise DriverError(
+                message=f"{exc}"
+            ) from exc
         except Exception as err:
-            raise DriverError(message=f"Unknown DataTable Error: {err}") from err
+            raise DriverError(
+                message=f"Unknown DataTable Error: {err}"
+            ) from err
         # is connected
         if self._connection:
             self._connected = True
             self._initialized_on = time.time()
         return self
 
     async def close(self):  # pylint: disable=W0221,W0236
         """
         Closing DeltaTable Connection
         """
         try:
-            pass  # TODO
+            self._connection = None
+            self._connected = False
+            self._delta = None
         except Exception as err:
-            raise DriverError(f"Unknown Closing Error: {err}") from err
+            raise DriverError(
+                f"Unknown Closing Error: {err}"
+            ) from err
 
     disconnect = close
 
     def load_version(self, version: Union[int, datetime]):
         if isinstance(version, int):
             self._connection.load_version(version)
         elif isinstance(version, datetime):
@@ -111,41 +133,49 @@
         except Exception as err:  # pylint: disable=W0703
             error = err
         finally:
             self.delete(key)
             return [result, error]  # pylint: disable=W0150
 
     async def create(
-        self, path: Union[str, Path], data: Any, name: Optional[str] = None, mode: str = "append", **kwargs
+        self,
+        path: Union[str, Path],
+        data: Any,
+        name: Optional[str] = None,
+        mode: str = "append",
+        **kwargs
     ):
         if isinstance(path, str):
             path = Path(str).resolve()
         if isinstance(data, str):
             data = Path(str).resolve()
         if isinstance(data, Path):
             # open this file with Pandas or Arrow
             ext = data.suffix
             if ext == ".csv":
                 read_options = pcsv.ReadOptions()
                 parse_options = pcsv.ParseOptions()
                 convert_options = pcsv.ConvertOptions()
                 data = pcsv.read_csv(
-                    data, read_options=read_options, parse_options=parse_options, convert_options=convert_options
+                    data,
+                    read_options=read_options,
+                    parse_options=parse_options,
+                    convert_options=convert_options
                 )
             elif ext in [".xls", ".xlsx"]:
                 if ext == ".xls":
                     engine = "xlrd"
                 else:
                     engine = "openpyxl"
                 data = pd.read_excel(data, engine=engine)
             elif ext == ".parquet":
                 data = pq.read_table(data)
         try:
             write_deltalake(path, data, name=name, mode=mode, **kwargs)
-        except PyDeltaTableError as exc:
+        except DeltaError as exc:
             raise DriverError(f"Delta: can't create a table in path {path}, error: {exc}") from exc
         except Exception as exc:
             raise DriverError(f"Delta Error: {exc}") from exc
 
     def execute(self, sentence: Any):  # pylint: disable=W0221,W0236
         raise NotImplementedError
 
@@ -179,46 +209,60 @@
             if factory == "pandas":
                 result = self._connection.to_pandas(**args)
             elif factory == "arrow":
                 result = self._connection.to_pyarrow_table(**args)
             elif factory == "arrow_dataset":
                 result = self._connection.to_pyarrow_dataset(**args, **kwargs)
             return result
-        except (PyDeltaTableError, DeltaTableProtocolError) as exc:
+        except (DeltaError, DeltaProtocolError) as exc:
             raise DriverError(f"DeltaTable Error: {exc}") from exc
         except Exception as exc:
             raise DriverError(f"Query Error: {exc}") from exc
 
     async def query(
         self,
         sentence: Optional[str] = None,
         partitions: Optional[list] = None,
-        columns: Optional[list] = None,
+        tablename: Optional[str] = "arrow_dataset",
         factory: Optional[str] = "pandas",
         **kwargs,
     ):  # pylint: disable=W0221,W0236
         """query.
-        Getting Data from Delta using a query (with DuckDB) or via columns and
-        partitions.
+        Getting Data from Delta using a query (with DuckDB)
         """
         result = None
         error = None
         args = {}
         if partitions:
             args = {"partitions": partitions}
-        if columns:
-            args["columns"] = columns
         try:
-            if factory == "pandas":
-                result = self._connection.to_pandas(**args)
-            elif factory == "arrow":
-                result = self._connection.to_pyarrow_table(**args)
-            elif factory == "arrow_dataset":
-                result = self._connection.to_pyarrow_dataset(**args, **kwargs)
-        except (PyDeltaTableError, DeltaTableProtocolError) as exc:
+            # connect to an in-memory database
+            con = duckdb.connect()
+            dataset = self._connection.to_pyarrow_dataset(**args, **kwargs)
+            ex_data = duckdb.arrow(dataset)
+            if sentence and sentence.strip().upper().startswith("SELECT"):
+                # Register the Arrow dataset as a table
+                con.register(tablename, dataset)
+                print('SENTENCE > ', sentence)
+                rst = con.execute(sentence)
+                if factory == "pandas":
+                    result = rst.df()
+                elif factory == "polars":
+                    result = rst.df_polars()
+                elif factory == 'arrow':
+                    result = rst.arrow()
+            else:
+                result = ex_data.filter(sentence)
+                if factory == "pandas":
+                    result = result.to_df()
+                elif factory == "polars":
+                    result = result.pl()
+                elif factory == 'arrow':
+                    result = result.to_arrow_table()
+        except (DeltaError, DeltaProtocolError) as exc:
             error = exc
             raise DriverError(f"DeltaTable Error: {exc}") from exc
         except Exception as exc:
             error = exc
             raise DriverError(f"Query Error: {exc}") from exc
         finally:
             return [result, error]  # pylint: disable=W0150
@@ -226,18 +270,24 @@
     fetch_all = query
 
     def queryrow(self, key: str, *args):  # pylint: disable=W0221,W0236
         return self.get(key, *args)
 
     fetch_one = queryrow
 
-    async def file_to_parquet(self, filename: Union[str, Path], parquet: str, factory: str = "pandas", **kwargs):
-        """csv_to_parquet.
+    async def file_to_parquet(
+        self,
+        filename: Union[str, Path],
+        parquet: str,
+        factory: str = "pandas",
+        **kwargs
+    ):
+        """file_to_parquet.
 
-        Creating a parquet file from a CSV object.
+        Creating a parquet file from a File (CSV/XLSX) object.
         """
         if isinstance(filename, str):
             filename = Path(filename).resolve()
         ext = filename.suffix
         arguments = kwargs.get("pd_args", {})
         df = None
         if ext in (".csv", ".txt", ".TXT", ".CSV"):
@@ -260,16 +310,119 @@
                 atable = pcsv.read_csv(filename, **arguments)
         elif ext in [".xls", ".xlsx"]:
             if ext == ".xls":
                 engine = "xlrd"
             else:
                 engine = "openpyxl"
             df = pd.read_excel(
-                filename, na_values=["NULL", "TBD"], na_filter=True, engine=engine, keep_default_na=False, **arguments
+                filename,
+                na_values=["NULL", "TBD"],
+                na_filter=True,
+                engine=engine,
+                keep_default_na=False,
+                **arguments
             )
         try:
             if df is not None:
                 df.to_parquet(parquet, engine="pyarrow", compression="snappy")
             elif atable is not None:
                 pq.write_table(atable, parquet, compression="snappy")
         except Exception as exc:
             raise DriverError(f"Query Error: {exc}") from exc
+
+    async def write(
+        self,
+        data:  Union[pd.DataFrame, dt.Frame, pl.DataFrame, Iterable],
+        table_id: str,
+        path: PurePath,
+        if_exists: str = "append",
+        partition_by: list = None,
+        **kwargs
+    ):
+        """write.
+        Writing Data into Delta Table.
+
+        Args:
+        - data: Data to be written,
+          it can be a Pandas DataFrame, a Polars DataFrame, a DataTable Frame or a list.
+        - table_id: Table Identifier
+        - path: Path to the Delta Table.
+        - if_exists: if_exists mode, default is "append", can be "error", "overwrite" or "ignore".
+        """
+        args = {
+            "mode": if_exists,
+            "engine": "rust",
+            **kwargs
+        }
+        if partition_by is not None:
+            args["partition_by"] = partition_by
+        try:
+            destination = path.joinpath(table_id)
+            if isinstance(data, pd.DataFrame):
+                write_deltalake(
+                    destination,
+                    data,
+                    **args
+                )
+            elif isinstance(data, (dt.Frame, pl.DataFrame)):
+                if isinstance(data, dt.Frame):
+                    data = pl.DataFrame(data.to_pandas())
+                data.write_delta(
+                    destination,
+                    **args
+                )
+            else:
+                # assuming a pyarrow:
+                write_deltalake(
+                    destination,
+                    data,
+                    **args
+                )
+            # Destination will be the new file path:
+            self._delta = destination
+        except (DeltaError, DeltaProtocolError) as exc:
+            raise DriverError(f"DeltaTable Error: {exc}") from exc
+        except Exception as exc:
+            raise DriverError(f"Query Error: {exc}") from exc
+
+    async def to_df(
+        self,
+        partitions: Optional[list] = None,
+        columns: Optional[list] = None,
+        factory: Optional[str] = "pandas",
+        **kwargs,
+    ):  # pylint: disable=W0221,W0236
+        """query.
+        Getting Delta Table into a Dataframe.
+
+        Args:
+        - partitions: List of Partitions.
+        - columns: List of Columns.
+        - factory: Factory to be used, default is "pandas", can be "arrow", "polars" or "datatable".
+        """
+        result = None
+        error = None
+        args = {}
+        if partitions:
+            args = {"partitions": partitions}
+        if columns:
+            args["columns"] = columns
+        try:
+            if factory == "pandas":
+                result = self._connection.to_pandas(**args)
+            elif factory == "arrow":
+                result = self._connection.to_pyarrow_table(**args)
+            elif factory == "arrow_dataset":
+                result = self._connection.to_pyarrow_dataset(**args, **kwargs)
+            elif factory == "polars":
+                table = self._connection.to_pyarrow_table(**args)
+                result = pl.from_arrow(table)
+        except (DeltaError, DeltaProtocolError) as exc:
+            error = exc
+            raise DriverError(f"DeltaTable Error: {exc}") from exc
+        except Exception as exc:
+            error = exc
+            raise DriverError(f"Query Error: {exc}") from exc
+        finally:
+            return [result, error]  # pylint: disable=W0150
+
+    fetch_all = query
```

## asyncdb/drivers/duckdb.py

```diff
@@ -210,19 +210,20 @@
                 self._connection.commit()
         except Exception as err:
             error = f"Error on Execute: {err}"
             raise DriverError(message=error) from err
         finally:
             return (result, error)
 
-    async def execute_many(self, sentence: Union[str, list], *args) -> Optional[Any]:
+    async def execute_many(self, sentence: Union[str, list], args: list) -> Optional[Any]:
         error = None
         await self.valid_operation(sentence)
         try:
-            result = self._connection.executemany(sentence, parameters=args)
+            print('THIS > ', sentence, args)
+            result = self._connection.executemany(sentence, args)
             if result:
                 self._connection.commit()
         except Exception as err:
             error = f"Error on Execute Many: {err}"
             raise DriverError(message=error) from err
         finally:
             return (result, error)
```

## Comparing `asyncdb-2.7.3.dist-info/LICENSE` & `asyncdb-2.7.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `asyncdb-2.7.3.dist-info/METADATA` & `asyncdb-2.7.4.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: asyncdb
-Version: 2.7.3
+Version: 2.7.4
 Summary: Library for Asynchronous data source connections     Collection of asyncio drivers.
 Home-page: https://github.com/phenobarbital/asyncdb
 Author: Jesus Lara
 Author-email: jesuslarag@gmail.com
 License: BSD
 Project-URL: Source, https://github.com/phenobarbital/asyncdb
 Project-URL: Funding, https://paypal.me/phenobarbital
@@ -78,26 +78,26 @@
 Requires-Dist: aiocouch ==3.0.0 ; extra == 'all'
 Requires-Dist: asyncmy ==0.2.9 ; extra == 'all'
 Requires-Dist: mysqlclient ==2.2.0 ; extra == 'all'
 Requires-Dist: aiomysql ==0.2.0 ; extra == 'all'
 Requires-Dist: pyspark ==3.5.0 ; extra == 'all'
 Requires-Dist: oracledb ==2.1.1 ; extra == 'all'
 Requires-Dist: hazelcast-python-client ==5.3.0 ; extra == 'all'
+Requires-Dist: deltalake ==0.17.4 ; extra == 'all'
 Requires-Dist: duckdb ==0.10.2 ; extra == 'all'
-Requires-Dist: deltalake ==0.13.0 ; extra == 'all'
 Requires-Dist: botocore ==1.31.64 ; extra == 'all'
 Requires-Dist: aiobotocore ==2.7.0 ; extra == 'all'
 Requires-Dist: aioboto3 ==12.0.0 ; extra == 'all'
 Requires-Dist: google-cloud-bigquery ==3.13.0 ; extra == 'all'
 Requires-Dist: google-cloud-storage ==2.16.0 ; extra == 'all'
-Requires-Dist: pandas-gbq ==0.19.2 ; extra == 'all'
+Requires-Dist: pandas-gbq ==0.22.0 ; extra == 'all'
 Requires-Dist: tqdm ==4.66.1 ; extra == 'all'
 Provides-Extra: bigquery
 Requires-Dist: google-cloud-bigquery ==3.13.0 ; extra == 'bigquery'
-Requires-Dist: pandas-gbq ==0.19.2 ; extra == 'bigquery'
+Requires-Dist: pandas-gbq ==0.22.0 ; extra == 'bigquery'
 Requires-Dist: google-cloud-storage ==2.16.0 ; extra == 'bigquery'
 Provides-Extra: boto3
 Requires-Dist: botocore ==1.31.64 ; extra == 'boto3'
 Requires-Dist: aiobotocore ==2.7.0 ; extra == 'boto3'
 Requires-Dist: aioboto3 ==12.0.0 ; extra == 'boto3'
 Provides-Extra: cassandra
 Requires-Dist: cassandra-driver ==3.29.1 ; extra == 'cassandra'
@@ -107,27 +107,28 @@
 Requires-Dist: dask ==2023.3.0 ; extra == 'dataframe'
 Requires-Dist: datatable ==1.1.0 ; extra == 'dataframe'
 Requires-Dist: python-datatable ==1.1.3 ; extra == 'dataframe'
 Requires-Dist: polars ==0.20.4 ; extra == 'dataframe'
 Requires-Dist: pyarrow ==16.0.0 ; extra == 'dataframe'
 Requires-Dist: connectorx ==0.2.3 ; extra == 'dataframe'
 Requires-Dist: pyspark ==3.5.0 ; extra == 'dataframe'
-Requires-Dist: deltalake ==0.13.0 ; extra == 'dataframe'
+Requires-Dist: deltalake ==0.17.4 ; extra == 'dataframe'
+Requires-Dist: duckdb ==0.10.2 ; extra == 'dataframe'
 Provides-Extra: default
 Requires-Dist: pylibmc ==1.6.3 ; extra == 'default'
 Requires-Dist: aiomcache ==0.8.1 ; extra == 'default'
 Requires-Dist: aiosqlite >=0.18.0 ; extra == 'default'
 Requires-Dist: cassandra-driver ==3.29.1 ; extra == 'default'
 Requires-Dist: rethinkdb ==2.4.10.post1 ; extra == 'default'
 Requires-Dist: influxdb ==5.3.1 ; extra == 'default'
 Requires-Dist: influxdb-client[async] ==1.39.0 ; extra == 'default'
 Requires-Dist: pymssql ==2.2.11 ; extra == 'default'
 Requires-Dist: redis ==5.0.1 ; extra == 'default'
+Requires-Dist: deltalake ==0.17.4 ; extra == 'default'
 Requires-Dist: duckdb ==0.10.2 ; extra == 'default'
-Requires-Dist: deltalake ==0.13.0 ; extra == 'default'
 Provides-Extra: elasticsearch
 Requires-Dist: elasticsearch[async] ==8.13.0 ; extra == 'elasticsearch'
 Provides-Extra: hazelcast
 Requires-Dist: hazelcast-python-client ==5.3.0 ; extra == 'hazelcast'
 Provides-Extra: influxdb
 Requires-Dist: influxdb ==5.3.1 ; extra == 'influxdb'
 Requires-Dist: influxdb-client[async] ==1.39.0 ; extra == 'influxdb'
```

## Comparing `asyncdb-2.7.3.dist-info/RECORD` & `asyncdb-2.7.4.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 asyncdb/__init__.py,sha256=0udlaJRz2alek7XfmdLNCM_kLB6NPY9szC5j4nsu3CU,411
-asyncdb/connections.py,sha256=MN_pNS66VqA3E_1gwa-U7eRwgbgmAOwYIVW7eZnFdtw,1297
+asyncdb/connections.py,sha256=qMLA_KBhvPYIym-pir8T8gqjIM5qktK9Friv808lHWI,1329
 asyncdb/interfaces.py,sha256=neAlRetp-gjwSSktJF-v3Z5vbJdgtODpTXgb1EKDIYk,25523
 asyncdb/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-asyncdb/version.py,sha256=Q_udHld690ZCsvAtXrwlox7A3Zjvsu4cX46JNu7nIn8,280
+asyncdb/version.py,sha256=WnwYfGyVkfWewOWPdpSTiRaXMIsMEb-RaWu4tc9W01k,280
 asyncdb/drivers/__init__.py,sha256=okzf2s2YG3qjE5Co21sdcsRPHsCedCc7lmhQjqT3OCY,28
 asyncdb/drivers/abstract.py,sha256=DFTeAdUC98t5n-1ASIrFnZQFr0kD3HMVC5SFc9r--6U,4529
-asyncdb/drivers/bigquery.py,sha256=MLLHpRCnRxzoGITxvuwy8bVPGMsHA8GGpvVATXPPryQ,17819
+asyncdb/drivers/bigquery.py,sha256=WD_ipr83mojibBSv_rqJxz_z2CIrVn6-JtLY_ydgeTM,19346
 asyncdb/drivers/cassandra.py,sha256=IVLklNnZ5IoZygL99zXyHLLEvfMkX_G6CQU1U3VaLtY,17209
-asyncdb/drivers/delta.py,sha256=laJLvumooUBlRRmobtS_TQLOI6lPhMGOopWX-v48s94,10172
-asyncdb/drivers/duckdb.py,sha256=Mp7pcCSpHcDaEUhYOaOQsovmaj2TjIXJVi1CFqujScg,10193
+asyncdb/drivers/delta.py,sha256=oXWU_3lOBqZOLyZcbiKsX7isOk0XyIaVFSEXw8HfQCI,14737
+asyncdb/drivers/duckdb.py,sha256=FKWpsD8CYBrH9aqi1pPwNv50K8ug-FVsgRQcbg9hVZA,10233
 asyncdb/drivers/dummy.py,sha256=hv4-_xDGc0vmYupoezxoA-afCJmTaSEn5jSTo12hGT8,2110
 asyncdb/drivers/hazel.py,sha256=aEXztGMbRJivEnIMeOn47KN1N5JLuVkbtUy9YE4q-Xg,13068
 asyncdb/drivers/influx.py,sha256=ag6hiy6cZP6qbXDWA3ax1_hjc4WYchvsKe5rYdaPLHU,20827
 asyncdb/drivers/jdbc.py,sha256=iUh4koHoEkK4nn2iE90Gqe4eRq-PtJo2TvXSZpdUFIQ,29764
 asyncdb/drivers/mcache.py,sha256=ZCta5ylKZwVvQZjAsKVwI90gRepHsqa2_CZ0bXIBj5s,6638
 asyncdb/drivers/memcache.py,sha256=9GYnWL5Q1hmBczQkiq_3jtcd-EnKeFmQqvFJvhnNE0Y,9336
 asyncdb/drivers/mongo.py,sha256=EvTlxjwoNQMu25QGM_MIwCAcf-bwpzBARVONspTrrHA,3316
@@ -43,27 +43,27 @@
 asyncdb/drivers/outputs/output.py,sha256=WgcgqrqwN0WSHpf4jTuTKftyH7Ov-KQjaWzAspR2TOc,986
 asyncdb/drivers/outputs/pandas.py,sha256=T94VGwsfQhq5P_o46TXTq6Av7E3vjPkxoBA4H4T4Z8k,1021
 asyncdb/drivers/outputs/polars.py,sha256=qJNPIu6AsBzSkSjMicLUwp20PnpvPA_Gx7fkHoPHtU4,864
 asyncdb/drivers/outputs/pyspark.py,sha256=ZoLiKGAcqO96NU2oapshn6rkxt-MKppeLlVWIsYq9IQ,1236
 asyncdb/drivers/outputs/record.py,sha256=sPFwhGgM9M1APINMzyQQJ6312MYqjNnauCVJWAbzC-A,1038
 asyncdb/drivers/outputs/recordset.py,sha256=oJpN0h2AaXmj6CbsOdN8uyUo3AC9YnD77GUqfAUOZsE,770
 asyncdb/exceptions/__init__.py,sha256=c6HhQu9AQcSn85Dp-jyvqiUUUVMK2gYJalN7gYdS008,867
-asyncdb/exceptions/exceptions.pypy39-pp73-win_amd64.pyd,sha256=yETCZpCnXBKZrWFfB9vBpOLOYt5QdcfxFTeuJZJCQC4,158720
+asyncdb/exceptions/exceptions.pypy39-pp73-win_amd64.pyd,sha256=qZDiRWl4e8Hq-y7oUJw2UDkZRGYnCrREWRmY2852CMs,158720
 asyncdb/exceptions/handlers.py,sha256=GEFCoNMkyXTBz9-tuJ3Y1Gsq2gcgAZcrQnzFZ4k_LVM,2539
 asyncdb/meta/__init__.py,sha256=Wl5uJSXic-gh52UKleUdEqdVaS2LTsSPsUOlh2nO3P8,176
 asyncdb/meta/record.py,sha256=bdKiYrlmX0FtBShvVw1rscMGfQq_CgDZwp194hHrCGg,3057
 asyncdb/meta/recordset.py,sha256=yRyKbCIg5kxDWgqsddMCTKQJctBfgvs6W9bOecaZg3k,2221
 asyncdb/models/__init__.py,sha256=I_v3ttQX7tdF6V2WlBh5bf6YUAxaukfHgiQmsE59cj8,542
 asyncdb/models/model.py,sha256=e02FQHXf2vEFOeMGpqO86mmzzNzHWOYMT6B2oXZP1No,17784
 asyncdb/utils/__init__.py,sha256=Bx7Kk7jHsVfVsNchJH5BdA4zDQFmvRxt9TnY0fd9qWo,136
 asyncdb/utils/functions.py,sha256=Pr5wkjacMvuG7vOSGGLF_CfKkVqTRZpnbJZcl6T2IpE,2484
 asyncdb/utils/modules.py,sha256=6tUulpoEUpitmoOd972JLdj_XiYx0xbHcAimZRZ81yE,655
-asyncdb/utils/types.pypy39-pp73-win_amd64.pyd,sha256=ki0v08dwzIp6s3BaFoQmk5uwQCsNIZzqT0sbJr93Sw4,111616
+asyncdb/utils/types.pypy39-pp73-win_amd64.pyd,sha256=B2ZBg3t-zQ7QeH9BAaQ8Vv_QTInETfZwhH1Z1cXVvCQ,111616
 asyncdb/utils/uv.py,sha256=r3HYjQmAAl4yOYQwvi2ghUGGJVWPqUFNqRWcjKWCNxs,319
 asyncdb/utils/encoders/__init__.py,sha256=YwtnLtxCmPm21EALjW1Y2upbyIHJiNWHjrchuetp4Fc,234
 asyncdb/utils/encoders/numpy.py,sha256=DkNoKLLA2haOG0qOm9J5RmRBkgiEZbcl9do5mBvIQbY,487
 asyncdb/utils/encoders/pg.py,sha256=i70nydH2S4XxAZUJdoJjXYsU_Ju6hAJbynLsztZSP10,1195
-asyncdb-2.7.3.dist-info/LICENSE,sha256=mThyqULh5pc9N8g3nfITEVMYcHOxzkPyJHQGWwFuQ_c,1538
-asyncdb-2.7.3.dist-info/METADATA,sha256=Jz18ZzAwpgjSY2iDEHI0Eh4GAq9CzUo7kc3Ltiei_Es,12805
-asyncdb-2.7.3.dist-info/WHEEL,sha256=1cqEyrKxUA_c-f0mOszCh_7q-AnzL9K6iMxWION_yb8,107
-asyncdb-2.7.3.dist-info/top_level.txt,sha256=vMLK2jQFYyNY3ta7idY3krhSki0RPpQ8yVZCVa724JI,8
-asyncdb-2.7.3.dist-info/RECORD,,
+asyncdb-2.7.4.dist-info/LICENSE,sha256=mThyqULh5pc9N8g3nfITEVMYcHOxzkPyJHQGWwFuQ_c,1538
+asyncdb-2.7.4.dist-info/METADATA,sha256=WBiFYR-1DYbityJKAnWlteQOod8AcKYHMm80EWho9YM,12860
+asyncdb-2.7.4.dist-info/WHEEL,sha256=1cqEyrKxUA_c-f0mOszCh_7q-AnzL9K6iMxWION_yb8,107
+asyncdb-2.7.4.dist-info/top_level.txt,sha256=vMLK2jQFYyNY3ta7idY3krhSki0RPpQ8yVZCVa724JI,8
+asyncdb-2.7.4.dist-info/RECORD,,
```

