# Comparing `tmp/akride-0.4.10-py3-none-any.whl.zip` & `tmp/akride-0.4.16-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,77 +1,77 @@
-Zip file size: 73000 bytes, number of entries: 75
--rw-r--r--  2.0 unx     2119 b- defN 24-Apr-16 06:31 akride/__init__.py
--rw-r--r--  2.0 unx     2499 b- defN 24-Apr-16 06:31 akride/background_task_manager.py
--rw-r--r--  2.0 unx    34742 b- defN 24-Apr-16 06:31 akride/client.py
--rw-r--r--  2.0 unx     1808 b- defN 24-Apr-16 06:31 akride/main.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/__init__.py
--rw-r--r--  2.0 unx     2904 b- defN 24-Apr-16 06:31 akride/_utils/background_task_helper.py
--rw-r--r--  2.0 unx     1357 b- defN 24-Apr-16 06:31 akride/_utils/class_executor.py
--rw-r--r--  2.0 unx     2624 b- defN 24-Apr-16 06:31 akride/_utils/exception_utils.py
--rw-r--r--  2.0 unx      584 b- defN 24-Apr-16 06:31 akride/_utils/file_utils.py
--rw-r--r--  2.0 unx    12931 b- defN 24-Apr-16 06:31 akride/_utils/job_creator.py
--rw-r--r--  2.0 unx      278 b- defN 24-Apr-16 06:31 akride/_utils/platform.py
--rw-r--r--  2.0 unx      454 b- defN 24-Apr-16 06:31 akride/_utils/progress_bar_helper.py
--rw-r--r--  2.0 unx     1459 b- defN 24-Apr-16 06:31 akride/_utils/proxy_utils.py
--rw-r--r--  2.0 unx      172 b- defN 24-Apr-16 06:31 akride/_utils/resource_utils.py
--rw-r--r--  2.0 unx      595 b- defN 24-Apr-16 06:31 akride/_utils/retry_helper.py
--rw-r--r--  2.0 unx      811 b- defN 24-Apr-16 06:31 akride/_utils/store_utils.py
--rw-r--r--  2.0 unx      473 b- defN 24-Apr-16 06:31 akride/_utils/workflow_helper.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/catalog/__init__.py
--rw-r--r--  2.0 unx      779 b- defN 24-Apr-16 06:31 akride/_utils/catalog/catalog_tables_helper.py
--rw-r--r--  2.0 unx      574 b- defN 24-Apr-16 06:31 akride/_utils/catalog/dataset_tables_info.py
--rw-r--r--  2.0 unx      194 b- defN 24-Apr-16 06:31 akride/_utils/catalog/enums.py
--rw-r--r--  2.0 unx      890 b- defN 24-Apr-16 06:31 akride/_utils/catalog/pipeline_tables_info.py
--rw-r--r--  2.0 unx      541 b- defN 24-Apr-16 06:31 akride/_utils/catalog/tables_info.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/pipeline/__init__.py
--rw-r--r--  2.0 unx      250 b- defN 24-Apr-16 06:31 akride/_utils/pipeline/constants.py
--rw-r--r--  2.0 unx     1159 b- defN 24-Apr-16 06:31 akride/_utils/pipeline/pipeline_helper.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/progress_manager/__init__.py
--rw-r--r--  2.0 unx     2037 b- defN 24-Apr-16 06:31 akride/_utils/progress_manager/manager.py
--rw-r--r--  2.0 unx     1646 b- defN 24-Apr-16 06:31 akride/_utils/progress_manager/progress_step.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/_utils/rest/__init__.py
--rw-r--r--  2.0 unx      528 b- defN 24-Apr-16 06:31 akride/_utils/rest/requests_session_manager.py
--rw-r--r--  2.0 unx     3468 b- defN 24-Apr-16 06:31 akride/_utils/rest/rest_client.py
--rw-r--r--  2.0 unx      194 b- defN 24-Apr-16 06:31 akride/_utils/rest/utils.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/__init__.py
--rw-r--r--  2.0 unx     1450 b- defN 24-Apr-16 06:31 akride/core/_log.py
--rw-r--r--  2.0 unx    15237 b- defN 24-Apr-16 06:31 akride/core/_pipeline_executor.py
--rw-r--r--  2.0 unx     1859 b- defN 24-Apr-16 06:31 akride/core/constants.py
--rw-r--r--  2.0 unx     2284 b- defN 24-Apr-16 06:31 akride/core/enums.py
--rw-r--r--  2.0 unx     1500 b- defN 24-Apr-16 06:31 akride/core/exceptions.py
--rw-r--r--  2.0 unx     9614 b- defN 24-Apr-16 06:31 akride/core/types.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/__init__.py
--rw-r--r--  2.0 unx    18527 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/catalog_manager.py
--rw-r--r--  2.0 unx    16073 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/dataset_manager.py
--rw-r--r--  2.0 unx    28206 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/job_manager.py
--rw-r--r--  2.0 unx     2238 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/manager.py
--rw-r--r--  2.0 unx     8702 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/resultset_manager.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-16 06:31 akride/core/_entity_managers/subscriptions_manager.py
--rw-r--r--  2.0 unx      140 b- defN 24-Apr-16 06:31 akride/core/_filters/__init__.py
--rw-r--r--  2.0 unx      253 b- defN 24-Apr-16 06:31 akride/core/_filters/enums.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/__init__.py
--rw-r--r--  2.0 unx     6412 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/ingest_partitioner_filter.py
--rw-r--r--  2.0 unx      696 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/models.py
--rw-r--r--  2.0 unx      288 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/partitioner_filter.py
--rw-r--r--  2.0 unx     6043 b- defN 24-Apr-16 06:31 akride/core/_filters/partitioners/process_partitioner_filter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/_filters/sink/__init__.py
--rw-r--r--  2.0 unx     1247 b- defN 24-Apr-16 06:31 akride/core/_filters/sink/models.py
--rw-r--r--  2.0 unx     9178 b- defN 24-Apr-16 06:31 akride/core/_filters/sink/sink_writer_filter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/conf/__init__.py
--rw-rw-r--  2.0 unx      493 b- defN 24-Apr-16 06:31 akride/core/conf/pylogconf.yaml
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/entities/__init__.py
--rw-r--r--  2.0 unx      796 b- defN 24-Apr-16 06:31 akride/core/entities/catalogs.py
--rw-r--r--  2.0 unx      736 b- defN 24-Apr-16 06:31 akride/core/entities/datasets.py
--rw-r--r--  2.0 unx     1549 b- defN 24-Apr-16 06:31 akride/core/entities/entity.py
--rw-r--r--  2.0 unx     5133 b- defN 24-Apr-16 06:31 akride/core/entities/jobs.py
--rw-r--r--  2.0 unx      436 b- defN 24-Apr-16 06:31 akride/core/entities/pipeline.py
--rw-r--r--  2.0 unx      955 b- defN 24-Apr-16 06:31 akride/core/entities/resultsets.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-16 06:31 akride/core/models/__init__.py
--rw-r--r--  2.0 unx      398 b- defN 24-Apr-16 06:31 akride/core/models/catalog_details.py
--rw-r--r--  2.0 unx      246 b- defN 24-Apr-16 06:31 akride/core/models/progress_info.py
--rw-rw-r--  2.0 unx      130 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     3475 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/WHEEL
--rw-r--r--  2.0 unx       44 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        7 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6753 b- defN 24-Apr-16 06:37 akride-0.4.10.dist-info/RECORD
-75 files, 230574 bytes uncompressed, 62086 bytes compressed:  73.1%
+Zip file size: 74166 bytes, number of entries: 75
+-rw-r--r--  2.0 unx     2119 b- defN 24-May-13 06:03 akride/__init__.py
+-rw-r--r--  2.0 unx     2499 b- defN 24-May-13 06:03 akride/background_task_manager.py
+-rw-r--r--  2.0 unx    35983 b- defN 24-May-13 06:03 akride/client.py
+-rw-r--r--  2.0 unx     2563 b- defN 24-May-13 06:03 akride/main.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/_utils/__init__.py
+-rw-r--r--  2.0 unx     2904 b- defN 24-May-13 06:03 akride/_utils/background_task_helper.py
+-rw-r--r--  2.0 unx     1357 b- defN 24-May-13 06:03 akride/_utils/class_executor.py
+-rw-r--r--  2.0 unx     2624 b- defN 24-May-13 06:03 akride/_utils/exception_utils.py
+-rw-r--r--  2.0 unx      584 b- defN 24-May-13 06:03 akride/_utils/file_utils.py
+-rw-r--r--  2.0 unx    14188 b- defN 24-May-13 06:03 akride/_utils/job_creator.py
+-rw-r--r--  2.0 unx      278 b- defN 24-May-13 06:03 akride/_utils/platform.py
+-rw-r--r--  2.0 unx      454 b- defN 24-May-13 06:03 akride/_utils/progress_bar_helper.py
+-rw-r--r--  2.0 unx     1459 b- defN 24-May-13 06:03 akride/_utils/proxy_utils.py
+-rw-r--r--  2.0 unx      172 b- defN 24-May-13 06:03 akride/_utils/resource_utils.py
+-rw-r--r--  2.0 unx      595 b- defN 24-May-13 06:03 akride/_utils/retry_helper.py
+-rw-r--r--  2.0 unx      811 b- defN 24-May-13 06:03 akride/_utils/store_utils.py
+-rw-r--r--  2.0 unx      473 b- defN 24-May-13 06:03 akride/_utils/workflow_helper.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/_utils/catalog/__init__.py
+-rw-r--r--  2.0 unx      779 b- defN 24-May-13 06:03 akride/_utils/catalog/catalog_tables_helper.py
+-rw-r--r--  2.0 unx      574 b- defN 24-May-13 06:03 akride/_utils/catalog/dataset_tables_info.py
+-rw-r--r--  2.0 unx      194 b- defN 24-May-13 06:03 akride/_utils/catalog/enums.py
+-rw-r--r--  2.0 unx      890 b- defN 24-May-13 06:03 akride/_utils/catalog/pipeline_tables_info.py
+-rw-r--r--  2.0 unx      541 b- defN 24-May-13 06:03 akride/_utils/catalog/tables_info.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/_utils/pipeline/__init__.py
+-rw-r--r--  2.0 unx      250 b- defN 24-May-13 06:03 akride/_utils/pipeline/constants.py
+-rw-r--r--  2.0 unx     1159 b- defN 24-May-13 06:03 akride/_utils/pipeline/pipeline_helper.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/_utils/progress_manager/__init__.py
+-rw-r--r--  2.0 unx     2037 b- defN 24-May-13 06:03 akride/_utils/progress_manager/manager.py
+-rw-r--r--  2.0 unx     1646 b- defN 24-May-13 06:03 akride/_utils/progress_manager/progress_step.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/_utils/rest/__init__.py
+-rw-r--r--  2.0 unx      528 b- defN 24-May-13 06:03 akride/_utils/rest/requests_session_manager.py
+-rw-r--r--  2.0 unx     3468 b- defN 24-May-13 06:03 akride/_utils/rest/rest_client.py
+-rw-r--r--  2.0 unx      194 b- defN 24-May-13 06:03 akride/_utils/rest/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/core/__init__.py
+-rw-r--r--  2.0 unx     1450 b- defN 24-May-13 06:03 akride/core/_log.py
+-rw-r--r--  2.0 unx    15237 b- defN 24-May-13 06:03 akride/core/_pipeline_executor.py
+-rw-r--r--  2.0 unx     1859 b- defN 24-May-13 06:03 akride/core/constants.py
+-rw-r--r--  2.0 unx     2309 b- defN 24-May-13 06:03 akride/core/enums.py
+-rw-r--r--  2.0 unx     1500 b- defN 24-May-13 06:03 akride/core/exceptions.py
+-rw-r--r--  2.0 unx     9614 b- defN 24-May-13 06:03 akride/core/types.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/core/_entity_managers/__init__.py
+-rw-r--r--  2.0 unx    18527 b- defN 24-May-13 06:03 akride/core/_entity_managers/catalog_manager.py
+-rw-r--r--  2.0 unx    16073 b- defN 24-May-13 06:03 akride/core/_entity_managers/dataset_manager.py
+-rw-r--r--  2.0 unx    30350 b- defN 24-May-13 06:03 akride/core/_entity_managers/job_manager.py
+-rw-r--r--  2.0 unx     2238 b- defN 24-May-13 06:03 akride/core/_entity_managers/manager.py
+-rw-r--r--  2.0 unx     8702 b- defN 24-May-13 06:03 akride/core/_entity_managers/resultset_manager.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-13 06:03 akride/core/_entity_managers/subscriptions_manager.py
+-rw-r--r--  2.0 unx      140 b- defN 24-May-13 06:03 akride/core/_filters/__init__.py
+-rw-r--r--  2.0 unx      253 b- defN 24-May-13 06:03 akride/core/_filters/enums.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/core/_filters/partitioners/__init__.py
+-rw-r--r--  2.0 unx     6412 b- defN 24-May-13 06:03 akride/core/_filters/partitioners/ingest_partitioner_filter.py
+-rw-r--r--  2.0 unx      696 b- defN 24-May-13 06:03 akride/core/_filters/partitioners/models.py
+-rw-r--r--  2.0 unx      288 b- defN 24-May-13 06:03 akride/core/_filters/partitioners/partitioner_filter.py
+-rw-r--r--  2.0 unx     6043 b- defN 24-May-13 06:03 akride/core/_filters/partitioners/process_partitioner_filter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/core/_filters/sink/__init__.py
+-rw-r--r--  2.0 unx     1247 b- defN 24-May-13 06:03 akride/core/_filters/sink/models.py
+-rw-r--r--  2.0 unx     9178 b- defN 24-May-13 06:03 akride/core/_filters/sink/sink_writer_filter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/core/conf/__init__.py
+-rw-rw-r--  2.0 unx      493 b- defN 24-May-13 06:03 akride/core/conf/pylogconf.yaml
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/core/entities/__init__.py
+-rw-r--r--  2.0 unx      796 b- defN 24-May-13 06:03 akride/core/entities/catalogs.py
+-rw-r--r--  2.0 unx      736 b- defN 24-May-13 06:03 akride/core/entities/datasets.py
+-rw-r--r--  2.0 unx     1549 b- defN 24-May-13 06:03 akride/core/entities/entity.py
+-rw-r--r--  2.0 unx     5565 b- defN 24-May-13 06:03 akride/core/entities/jobs.py
+-rw-r--r--  2.0 unx      436 b- defN 24-May-13 06:03 akride/core/entities/pipeline.py
+-rw-r--r--  2.0 unx      955 b- defN 24-May-13 06:03 akride/core/entities/resultsets.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-13 06:03 akride/core/models/__init__.py
+-rw-r--r--  2.0 unx      398 b- defN 24-May-13 06:03 akride/core/models/catalog_details.py
+-rw-r--r--  2.0 unx      246 b- defN 24-May-13 06:03 akride/core/models/progress_info.py
+-rw-rw-r--  2.0 unx      130 b- defN 24-May-13 06:08 akride-0.4.16.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     3490 b- defN 24-May-13 06:08 akride-0.4.16.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-13 06:08 akride-0.4.16.dist-info/WHEEL
+-rw-r--r--  2.0 unx       44 b- defN 24-May-13 06:08 akride-0.4.16.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 24-May-13 06:08 akride-0.4.16.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6753 b- defN 24-May-13 06:08 akride-0.4.16.dist-info/RECORD
+75 files, 236443 bytes uncompressed, 63252 bytes compressed:  73.2%
```

## zipnote {}

```diff
@@ -201,26 +201,26 @@
 
 Filename: akride/core/models/catalog_details.py
 Comment: 
 
 Filename: akride/core/models/progress_info.py
 Comment: 
 
-Filename: akride-0.4.10.dist-info/LICENSE.txt
+Filename: akride-0.4.16.dist-info/LICENSE.txt
 Comment: 
 
-Filename: akride-0.4.10.dist-info/METADATA
+Filename: akride-0.4.16.dist-info/METADATA
 Comment: 
 
-Filename: akride-0.4.10.dist-info/WHEEL
+Filename: akride-0.4.16.dist-info/WHEEL
 Comment: 
 
-Filename: akride-0.4.10.dist-info/entry_points.txt
+Filename: akride-0.4.16.dist-info/entry_points.txt
 Comment: 
 
-Filename: akride-0.4.10.dist-info/top_level.txt
+Filename: akride-0.4.16.dist-info/top_level.txt
 Comment: 
 
-Filename: akride-0.4.10.dist-info/RECORD
+Filename: akride-0.4.16.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## akride/client.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 
 import json
 from contextlib import suppress
 from typing import Any, Dict, List, Optional, Tuple, Union
 
@@ -669,14 +669,15 @@
         embed_algo: Union[str, EmbedAlgoType] = EmbedAlgoType.UMAP,
         num_clusters: Optional[int] = None,
         max_images: int = 1000,
         catalog_table: Optional[CatalogTable] = None,
         analyze_params: Optional[AnalyzeJobParams] = None,
         pipeline: Optional[Pipeline] = None,
         filters: List[Condition] = None,  # type: ignore
+        reference_job: Job = None,
     ) -> JobSpec:
         """
         Creates a JobSpec object that specifies how a job is to be created.
 
         Parameters:
         -----------
         dataset: Dataset
@@ -705,37 +706,48 @@
             a dataset is created.
             default: "primary"
         analyze_params: AnalyzeJobParams, optional
             Analyze job related configuration parameters
         filters : List[Condition], optional
             The filters to be used to select a subset of samples for this job.
             These filters are applied to the catalog specified by catalog_name.
+        reference_job: Job, optional
+            The reference job for this compare job
         """
         if pipeline is None:
             pipelines: List[Pipeline] = self.get_attached_pipelines(
                 dataset=dataset
             )
             pipeline = self.datasets._get_default_image_pipeline(
                 attached_pipelines=pipelines
             )
         if catalog_table is None:
             catalog_table = CatalogTable(table_name="primary")
+
+        is_compare = False
+
+        if job_type == JobType.COMPARE:
+            is_compare = True
+            job_type = JobType.EXPLORE
+
         return JobSpec(
             dataset=dataset,
             job_type=job_type,
             job_name=job_name,
             predictions_file=predictions_file,
             cluster_algo=cluster_algo,
             embed_algo=embed_algo,
             num_clusters=num_clusters,
             max_images=max_images,
             catalog_table=catalog_table,
             filters=filters,
             pipeline=pipeline,
             analyze_params=analyze_params,
+            is_compare=is_compare,
+            reference_job=reference_job,
         )
 
     def create_job(self, spec: JobSpec) -> Job:
         """
         Creates an explore job for the specified dataset.
 
         Parameters:
@@ -804,14 +816,46 @@
         Returns
         -------
         List[Entity]
             A list of Entity objects representing jobs.
         """
         return self.jobs.get_entities(attributes)  # type: ignore
 
+    def get_compatible_reference_jobs(
+        self,
+        dataset: Dataset,
+        pipeline: Pipeline,
+        catalog_table: CatalogTable,
+        search_key: str = None,
+    ) -> List[Job]:
+        """
+        Retrieves jobs created from a given catalog_table which can be used to
+        create “JobType.COMPARE” job types
+
+        Parameters
+        ----------
+        dataset: Dataset
+            The dataset to explore.
+        pipeline: Pipeline
+            The pipeline to use.
+        catalog_table:
+            The catalog table to use for creating compare job.
+        search_key: str
+            Filter jobs across fields like job name
+
+        Returns
+        -------
+        List[Entity]
+            A list of Entity objects representing jobs.
+        """
+
+        return self.jobs.get_compatible_reference_jobs(
+            dataset, pipeline, catalog_table, search_key
+        )
+
     def get_thumbnail_images(
         self, samples: SampleInfoList
     ) -> List[Image.Image]:
         """
         Retrieves the thumbnail images corresponding to the samples.
 
         Parameters
```

## akride/main.py

```diff
@@ -8,27 +8,37 @@
     parser = argparse.ArgumentParser(description="Akride ingestion utility.")
     sub_parser = parser.add_subparsers(
         help="sub command", dest="command", required=True
     )
     ingest_parser = sub_parser.add_parser("ingest", help="ingest data")
 
     ingest_parser.add_argument(
-        "-d", "--dataset_name", required=True, type=str, help="Dataset Name"
+        "-n", "--dataset_name", type=str, help="Dataset Name"
+    )
+
+    ingest_parser.add_argument(
+        "-d", "--dataset_id", type=str, help="Dataset Id"
     )
 
     ingest_parser.add_argument(
         "-f",
         "--featurizer_type",
+        choices=["patch", "full"],
         default="patch",
         type=str,
         help="Featurizer type",
     )
 
     ingest_parser.add_argument(
-        "-c", "--with_clip", default="yes", type=str, help="CLIP needed"
+        "-c",
+        "--with_clip",
+        choices=["yes", "no"],
+        default="yes",
+        type=str,
+        help="CLIP needed",
     )
 
     ingest_parser.add_argument(
         "-i",
         "--input_dir",
         required=True,
         type=str,
@@ -41,25 +51,41 @@
     ingest_parser.add_argument(
         "-a", "--api_key", required=True, type=str, help="Api key"
     )
 
     # Parse arguments
     args = parser.parse_args()
     dataset_name = args.dataset_name
+    dataset_id = args.dataset_id
     featurizer_type = args.featurizer_type
     with_clip = args.with_clip
     input_dir = args.input_dir
     endpoint = args.endpoint
     api_key = args.api_key
 
+    if not dataset_name and not dataset_id:
+        raise Exception(
+            "one of the arguments -n/--dataset_name -d/--dataset_id is required"
+        )
+
     # Initialize client
     de_client = AkriDEClient(saas_endpoint=endpoint, api_key=api_key)
 
-    # Fetch dataset by name
-    dataset = de_client.get_dataset_by_name(dataset_name)
+    if dataset_id:
+        datasets = de_client.get_datasets(
+            attributes={"filter_by_ids": [dataset_id]}
+        )
+        if not datasets:
+            raise Exception(f"Dataset with id `{dataset_id}` is not found")
+        dataset = datasets[0]
+    else:
+        # Fetch dataset by name
+        dataset = de_client.get_dataset_by_name(dataset_name)
+        if not dataset:
+            raise Exception(f"Dataset with name `{dataset_name}` is not found")
 
     # Start ingestion
     de_client.ingest_dataset(
         dataset=dataset,
         data_directory=input_dir,
         use_patch_featurizer=featurizer_type == "patch",
         with_clip_featurizer=with_clip == "yes",
```

## akride/_utils/background_task_helper.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 
 import threading
 from typing import Optional
 
 from akride._utils.progress_manager.manager import ProgressManager
```

## akride/_utils/exception_utils.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 from functools import wraps
 from typing import Callable, Union
 
 from akridata_akrimanager_v2 import ApiException as AMException
 from akridata_dsp import ApiException as DSException
```

## akride/_utils/job_creator.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 
 import time
 from typing import Optional, Tuple
 
 import akridata_akrimanager_v2 as am
@@ -23,14 +23,15 @@
     DatasetInfo,
     EmbedderQuality,
     RequestDataSourceCreate,
     RequestSourceMetaJobRequestCreate,
     SubmitRequestTunables,
 )
 
+from akride.core.entities.jobs import Job
 from akride.core.enums import ClusterAlgoType, EmbedAlgoType, JobType
 from akride.core.exceptions import UserError
 from akride.core.types import AnalyzeJobParams, CatalogDetails, CatalogTable
 
 
 class JobCreator:  # pylint: disable=too-few-public-methods
     """Create a ds job"""
@@ -59,14 +60,16 @@
         catalog_table: CatalogTable,
         pipeline_id: str,
         embedder_quality: str = EmbedderQuality.HIGH,
         cluster_first: bool = False,
         max_images: int = 1000,
         num_clusters: Optional[int] = None,
         analyze_params: Optional[AnalyzeJobParams] = None,
+        is_compare: bool = False,
+        reference_job: Job = None,
     ) -> Tuple[CreateJobRequest, CreateJobRequestResponse]:
         """_summary_
 
         Args:
             job_type (str): Job Type
             job_name (str): Name of the Job
             dataset_id (str): Dataset ID
@@ -78,14 +81,16 @@
             cluster_first (bool, optional): If true clustering is done before
             embedding. Defaults to True.
             table_name (str, optional): Catalog table name. Defaults to "primary".
             max_images (int, optional): Max Images to use in the job. Defaults to 1000.
             num_clusters (Optional[int], optional): Number of clusters to
             create. Defaults to None.
             analyze_params: AnalyzeParams, optional
+            is_compare: bool, optional: True if it is a compare job
+            reference_job: reference job details
             Additional params for Analyze job
 
         Raises:
             ValueError: Invalid input
 
         Returns:
             Tuple[CreateJobRequest, CreateJobRequestResponse]: request and response
@@ -94,15 +99,15 @@
             self.catalog_api.get_catalog_tables(dataset_id=dataset_id)
         )  # type: ignore
 
         required_pipeline: CatalogPipelineTable = self._find_required_pipeline(
             catalog_tables, pipeline_id=pipeline_id
         )
         if catalog_table.is_view:
-            view_details = self._find_required_view(
+            view_details = self.find_required_view(
                 dataset_id=dataset_id, catalog_table=catalog_table
             )
             query_id = self._execute_view_query(
                 view_id=view_details.view_id,  # type: ignore
                 view_name=view_details.view_name,  # type: ignore
                 limit=max_images,
             )
@@ -134,14 +139,16 @@
             job_type=job_type,
             clusterer_algo=clusterer_algo,
             embedder_algo=embedder_algo,
             embedder_quality=embedder_quality,
             cluster_first=cluster_first,
             num_clusters=num_clusters,
             analyze_params=analyze_params,
+            is_compare=is_compare,
+            reference_job=reference_job,
         )
         resp = self.request_api.create_job_request(
             create_job_request=request
         )  # type: ignore
         return request, resp  # type: ignore
 
     def _get_job_request(  # pylint: disable=too-many-arguments
@@ -153,26 +160,30 @@
         query_id: str,
         clusterer_algo: str,
         embedder_algo: str,
         embedder_quality: str,
         cluster_first: bool,
         num_clusters: Optional[int] = None,
         analyze_params: Optional[AnalyzeJobParams] = None,
+        is_compare: bool = False,
+        reference_job: Job = None,
     ) -> CreateJobRequest:
         ds_req = DatasetInfo(dataset_id=dataset_id, pipeline_id=pipeline_id)
         req_data = RequestDataSourceCreate(catalog_filters=True)
         req_meta = RequestSourceMetaJobRequestCreate(vcs_query_id=query_id)
         submit_params = self._get_submit_params(
             job_type=job_type,
             clusterer_algo=clusterer_algo,
             embedder_algo=embedder_algo,
             embedder_quality=embedder_quality,
             cluster_first=cluster_first,
             num_clusters=num_clusters,
             analyze_params=analyze_params,
+            is_compare=is_compare,
+            reference_job=reference_job,
         )
         request = CreateJobRequest(
             dataset=ds_req,
             reqname=job_name,
             request_source_meta=req_meta,
             src=req_data,
             submit_params=submit_params,
@@ -184,22 +195,26 @@
         job_type: str = JobType.EXPLORE,
         clusterer_algo: str = ClusterAlgoType.HDBSCAN,
         embedder_algo: str = EmbedAlgoType.UMAP,
         embedder_quality: str = EmbedderQuality.HIGH,
         cluster_first: bool = True,
         num_clusters: Optional[int] = None,
         analyze_params: Optional[AnalyzeJobParams] = None,
+        is_compare: bool = False,
+        reference_job: Job = None,
     ) -> CreateSubmitJobRequest:
-        tunables = SubmitRequestTunables(
-            cluster_algo=clusterer_algo,
-            cluster_first=cluster_first,
-            embedder_algo=embedder_algo,
-            embedder_quality=embedder_quality,
-            nclusters=num_clusters,
-        )
+        tunables = None
+        if not is_compare:
+            tunables = SubmitRequestTunables(
+                cluster_algo=clusterer_algo,
+                cluster_first=cluster_first,
+                embedder_algo=embedder_algo,
+                embedder_quality=embedder_quality,
+                nclusters=num_clusters,
+            )
         datasource = None
         if JobType.is_analyze_job(job_type=job_type):
             catalog_db: dsp.CatalogDbAnalyzeMetaData = dsp.CatalogDbAnalyzeMetaData(
                 field_columns=self._get_analyze_field_column_map(
                     az_config=analyze_params.catalog_config  # type: ignore
                 )
             )
@@ -216,16 +231,36 @@
                         iou_config=dsp.AnalyzeMetaConfigItemRequest(
                             levels=analyze_params.iou_config
                         ),
                         plot_featurizer=analyze_params.plot_featurizer,
                     )
                 )
             )
+
+        src = None
+        request_source_meta = None
+
+        if is_compare:
+            src: dsp.RequestDataSourceSubmit = dsp.RequestDataSourceSubmit(
+                compare=True
+            )
+            request_source_meta: dsp.RequestSourceMetaJobRequestSubmit = (
+                dsp.RequestSourceMetaJobRequestSubmit(
+                    compare=dsp.CompareSourceMeta(
+                        ref_request_id=reference_job.get_id(),
+                        ref_request_owner=reference_job.ownername,
+                    )
+                )
+            )
         return CreateSubmitJobRequest(
-            job_type=job_type, tunables=tunables, data_source=datasource
+            job_type=job_type,
+            tunables=tunables,
+            data_source=datasource,
+            src=src,
+            request_source_meta=request_source_meta,
         )
 
     def _wait_query_completion(self, query_id: str):
         query_resp: QueryResponse = None  # type: ignore
         retries = 0
         while True:
             query_resp = self.qms_api.get_query(
@@ -253,15 +288,15 @@
         for pipeline in catalog_tables.pipelines:
             if pipeline.pipeline_id == pipeline_id:
                 required_pipeline = pipeline
         if required_pipeline is None:
             raise ValueError(f"pipeline {pipeline_id} not found")
         return required_pipeline
 
-    def _find_required_view(
+    def find_required_view(
         self, dataset_id, catalog_table: CatalogTable
     ) -> am.ViewResponse:
         all_views: am.ListViewsResponse = self.views_api.list_views(
             dataset_id=dataset_id
         )  # type: ignore
         view_of_interest = [
             record
```

## akride/_utils/proxy_utils.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 from typing import Dict, Optional, Tuple
 from urllib.request import getproxies, proxy_bypass
 
 import attr
 from yarl import URL
```

## akride/_utils/retry_helper.py

```diff
@@ -1,14 +1,14 @@
 from urllib3 import Retry
 
 
 def get_http_retry(
     total=4,
     backoff_factor=1,
-    status_forcelist=frozenset(range(500, 600)),
+    status_forcelist=frozenset(range(503, 600)),
     allowed_methods=frozenset(
         ["HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE", "POST", "PATCH"]
     ),
 ) -> Retry:
     """
     get_http_retry returns a Retry object which retries on 5XX status code
     for "HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE", "POST", "PATCH"
```

## akride/core/_log.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 import logging
 import logging.config
 import yaml
 
 from akride.core.constants import Constants
```

## akride/core/enums.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 
 from enum import Enum
 
 import akridata_akrimanager_v2 as am
 import akridata_dsp as dsp
@@ -24,14 +24,16 @@
     IMAGE = "image/*"
     VIDEO = "video/*"
 
 
 class JobType(dsp.JobType):
     """Supported Job types"""
 
+    COMPARE = "COMPARE"
+
     @classmethod
     def is_analyze_job(cls, job_type) -> bool:
         if job_type in [
             cls.ANALYZE_CLASSIFICATION,
             cls.ANALYZE_OBJECT_DETECTION,
             cls.ANALYZE_SEGMENTATION,
         ]:
```

## akride/core/exceptions.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 
 
 class BaseError(Exception):
     """
     Base class for creating custom exception classes.
```

## akride/core/types.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 
 
 import pprint
 from dataclasses import dataclass
 from typing import Dict, List, Optional
```

## akride/core/_entity_managers/job_manager.py

```diff
@@ -137,26 +137,30 @@
         catalog_table: CatalogTable,
         pipeline: Pipeline,
         num_clusters: Optional[int] = None,
         analyze_params: Optional[AnalyzeJobParams] = None,
         max_images: int = 1000,
         predictions_file: str = "",
         filters: Optional[List[Condition]] = None,
+        is_compare: bool = False,
+        reference_job: Job = None,
     ) -> Optional[Job]:
         request, response = self.job_creator.create_job(
             job_type=job_type,
             job_name=job_name,
             dataset_id=dataset.get_id(),  # type: ignore
             pipeline_id=pipeline.get_id(),  # type: ignore
             clusterer_algo=cluster_algo,
             embedder_algo=embed_algo,
             catalog_table=catalog_table,
             max_images=max_images,
             num_clusters=num_clusters,
             analyze_params=analyze_params,
+            is_compare=is_compare,
+            reference_job=reference_job,
         )
         job = Job(info=request)  # type: ignore
         job.id = response.rid
         job.name = job_name
         return job
 
     @translate_api_exceptions
@@ -192,14 +196,67 @@
             job = Job(info)
             job.id = info.reqid
             job.name = info.reqname
             job_list.append(job)
         return job_list
 
     @translate_api_exceptions
+    def get_compatible_reference_jobs(
+        self,
+        dataset: Dataset,
+        pipeline: Pipeline,
+        catalog_table: CatalogTable,
+        search_key: str = None,
+    ) -> List[Job]:
+        """
+        Retrieves jobs created from a given catalog_table which can be used to
+        create “JobType.COMPARE” job types
+
+        Parameters
+        ----------
+        dataset: Dataset
+            The dataset to explore.
+        pipeline: Pipeline
+            The pipeline to use.
+        catalog_table:
+            The catalog table to use for creating compare job.
+        search_key: str
+            Filter jobs across fields like job name
+
+        Returns
+        -------
+        List[Entity]
+            A list of Entity objects representing jobs.
+        """
+        vcs_view_id = None
+        if catalog_table.is_view:
+            view_details = self.job_creator.find_required_view(
+                dataset_id=dataset.id, catalog_table=catalog_table
+            )
+            vcs_view_id = view_details.view_id
+
+        api_response = self.request_api.get_compatible_ref_jobs_on_query(
+            dataset_id=dataset.id,
+            pipeline_id=pipeline.id,
+            vcs_catalog_table_name=catalog_table.table_name
+            if not catalog_table.is_view
+            else None,
+            vcs_view_id=vcs_view_id,
+            search_key=search_key,
+            archived=True,
+        )
+        job_list = []
+        for info in api_response.requests:
+            job = Job(info)
+            job.id = info.reqid
+            job.name = info.reqname
+            job_list.append(job)
+        return job_list
+
+    @translate_api_exceptions
     def get_thumbnail_images(
         self, samples: SampleInfoList
     ) -> List[Image.Image]:
         """
         Retrieves the thumbnail images for the provided job.
         Parameters
         ----------
@@ -241,14 +298,23 @@
         Returns
         -------
         List[Image.Image]
             A list of images.
         """
         job_id = samples.job_id
         images = dsp.Thumbnails(samples.get_point_ids())
+        req_details: (
+            GetDatasetRequestResponse
+        ) = self.request_api.get_request_details(
+            rid=job_id,
+        )  # type: ignore
+        if not req_details.dataset_is_highres_accessible:
+            raise UserError(
+                "Full resolution images are not accessible for this dataset"
+            )
         api_response = self.request_api.get_images(rid=job_id, images=images)
         result = []
         # TODO use the async API instead
         assert api_response is not None
         for image_path in api_response.data:  # type: ignore
             image_path = image_path.replace("/ds/images/", "")
             image_response = self.image_fetch_api.fetch_image(
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## akride/core/_entity_managers/subscriptions_manager.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 from typing import Any, Dict, List, Optional
 
 import akridata_akrimanager_v2 as am
 from akridata_akrimanager_v2 import DeSoftwareSpec, DeSoftwareSpecResponse
```

## akride/core/entities/catalogs.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 
 import akridata_akrimanager_v2 as am
 
 from akride.core.entities.entity import Entity
```

## akride/core/entities/datasets.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 
 import akridata_akrimanager_v2 as am
 
 from akride.core.entities.entity import Entity
```

## akride/core/entities/jobs.py

```diff
@@ -1,12 +1,12 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
-from typing import Dict
+from typing import Dict, Union
 
 import akridata_dsp as dsp
 
 from akride.core.entities.datasets import Dataset
 from akride.core.entities.entity import Entity
 from akride.core.types import AnalyzeJobParams, CatalogTable, PlotFeaturizer
 
@@ -56,14 +56,18 @@
             to the internal primary catalog that is created automatically when
             a dataset is created.
         filters : List[Condition], optional
             The filters to be used to select a subset of examples for this job.
             These filters are applied to the catalog specified by catalog_name.
         analyze_params: AnalyzeParams, optional
             Additional params for Analyze job
+        is_compare: bool, optional
+            Whether this is a compare job
+        reference_job: Job, optional
+            The reference job for this compare job
         """
         defaults = {
             "dataset": dataset,
             "job_type": JobType.EXPLORE,
             "predictions_file": "",
             "job_name": "",
             "cluster_algo": ClusterAlgoType.HDBSCAN,
@@ -93,21 +97,26 @@
                 self["job_type"] == JobType.ANALYZE_SEGMENTATION
                 or self["job_type"] == JobType.ANALYZE_OBJECT_DETECTION
             ):
                 params.plot_featurizer = PlotFeaturizer.CONTENT
         if self["pipeline"] is None:
             raise ValueError("Pipeline is not specified")
 
+        if self["is_compare"] and not self["reference_job"]:
+            raise ValueError("Reference job is required to create compare job")
+
 
 class Job(Entity):
     """
     Class representing a job entity.
     """
 
-    def __init__(self, info: dsp.CreateJobRequestResponse):
+    def __init__(
+        self, info: Union[dsp.CreateJobRequestResponse, dsp.CompatibleRefJob]
+    ):
         """
         Constructor for the Job class.
 
         Parameters
         ----------
         info : dsp.models.dataset_job_request.DatasetJobRequest
             The job request object.
@@ -151,7 +160,11 @@
 
         :raises ValueError: If job details are not available
         :return: int
         """
         if self.info:
             return self.info.to_dict()["tunables_default"]["max_clusters"]
         raise ValueError("job details are not unavailable")
+
+    @property
+    def ownername(self):
+        return self.info.ownername
```

## akride/core/entities/resultsets.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2023, Akridata, Inc - All Rights Reserved.
+ Copyright (C) 2024, Akridata, Inc - All Rights Reserved.
  Unauthorized copying of this file, via any medium is strictly prohibited
 """
 
 import akridata_dsp as dsp
 
 from akride.core.entities.entity import Entity
```

## Comparing `akride-0.4.10.dist-info/METADATA` & `akride-0.4.16.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: akride
-Version: 0.4.10
+Version: 0.4.16
 Summary: Data Explorer Client SDK
 Home-page: https://github.com/akridata-ai/akride-examples
 License: Proprietary
 Project-URL: Documentation, https://akridata-akride.readthedocs-hosted.com/en/latest/
 Platform: any
 Classifier: Development Status :: 4 - Beta
 Classifier: Programming Language :: Python
@@ -106,8 +106,8 @@
 
 ---
 
 For more information about AkriData, please visit [akridata.ai](https://www.akridata.ai).
 
 
 ## Docker command to ingest data
-docker run -it -v ${PWD}/data:/data akridata/akride:0.4.8  akride ingest -d <dataset_name> -i /data -e <de_endpoint> -a  <api_key>
+docker run -it -v ${PWD}/data:/data akridata/akride:latest akride ingest -n <dataset_name> -d <dataset_id> -i /data -e <de_endpoint> -a <api_key>
```

## Comparing `akride-0.4.10.dist-info/RECORD` & `akride-0.4.16.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 akride/__init__.py,sha256=_4QgfqdhPVgQ9Yl2QaPM_JYBfed0j5BxoIGN7tYkNeg,2119
 akride/background_task_manager.py,sha256=f5mMGM8Ji8VxYGV8S7JRU2dkr4ygFVwiGK_pgnVNZik,2499
-akride/client.py,sha256=1NQBrpBi19iGO83shnh2XihV0yxZDr3m4-UHiwhmXVs,34742
-akride/main.py,sha256=qRIgKovrjcIv0eZt4Sc63S60myEHjcr98u44DnpzmFM,1808
+akride/client.py,sha256=YiehxpjHTh7ZIdZqPEMYjXaCs1mRxsXZqO4oB8Ko8MU,35983
+akride/main.py,sha256=LS-Y2ucIn_GTzd73Tw0ruCsg4vMxQvFLgLpKSOclMCo,2563
 akride/_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-akride/_utils/background_task_helper.py,sha256=No8KJAeitnKAx_OkWTPk6hQlSOIbu1mdvO6WP71xv08,2904
+akride/_utils/background_task_helper.py,sha256=Yl-LOZ9abwVA-0owda81lrEgRNLkl4kJ2xF1sNrJFSo,2904
 akride/_utils/class_executor.py,sha256=y7qV7IRDiz0Gb1BM5oI2s7pcBUObj3B46mHdiTwc62o,1357
-akride/_utils/exception_utils.py,sha256=MvimxHZts3hxRfuDjNLtPcNnXs-Vh-IR33nxD34YKYA,2624
+akride/_utils/exception_utils.py,sha256=ElOf5shueLzmC0YCD1p7PmsPhN1ZWXWZDZ56Frp4Q5k,2624
 akride/_utils/file_utils.py,sha256=zmiooZXaFziN6jaK4b3aI1hDKWUQqN8LWguew-I0_Gw,584
-akride/_utils/job_creator.py,sha256=J4z9V7zaFsf_jY1DJNKigRdneYvaKI0fhzDDCtmDbvE,12931
+akride/_utils/job_creator.py,sha256=0vPVSr51N_4cspxz_TAuCL58ZHyuXQbFl0Px8cIp5-s,14188
 akride/_utils/platform.py,sha256=nutg_fRstZOGVryc3L78tZeuSG3H8eH_c3Pjyt41gIA,278
 akride/_utils/progress_bar_helper.py,sha256=68mwZAUFYM372AMEbNve5r7Tip4CUHUpkxYA1amp2AA,454
-akride/_utils/proxy_utils.py,sha256=1FBXCnW11iTInCcIyhKGGe9Z8TF67WRPSZWMvgVI2pM,1459
+akride/_utils/proxy_utils.py,sha256=QgRh4HGXisU0sbp-xN8eDvtS5Ic6HGgSKQ9ZRlaRE_k,1459
 akride/_utils/resource_utils.py,sha256=rN00dPs3vxu73m27xTeC-KWsRg6B0jTykkb_KKemeQ0,172
-akride/_utils/retry_helper.py,sha256=qcxSfJIRLHuJy9zABkSRJXTkhxeLNJVoH4e-BMXEIGg,595
+akride/_utils/retry_helper.py,sha256=80_KOJPAGaSKxduWSsQoim0RqA1W_gUjmomrFe-dT1U,595
 akride/_utils/store_utils.py,sha256=8FDHFXACLAh8jNKNfjkhCyJ2j89LZ8V6uKz7zeNYAxQ,811
 akride/_utils/workflow_helper.py,sha256=9oCxx4cU1UcClMaEB5jMe5NZXRChg4gXGXVJTpsimSQ,473
 akride/_utils/catalog/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/_utils/catalog/catalog_tables_helper.py,sha256=V6CPCRLD50yTfc02dU9eB3IbvSh1vfx8rgWfqqZT3do,779
 akride/_utils/catalog/dataset_tables_info.py,sha256=em9hogSeLs1xzsQNo3ITiEGGyZzHZziJDE09LQmhZlE,574
 akride/_utils/catalog/enums.py,sha256=2pV69NLp4Z4dQHgM7prOFXyiLv1PR0Car_7fCn_6Fik,194
 akride/_utils/catalog/pipeline_tables_info.py,sha256=0HvVx9BdCgqNkESveqEcEhEsOj3BOJVrtOhanO0p5hs,890
@@ -28,48 +28,48 @@
 akride/_utils/progress_manager/manager.py,sha256=CxqdeYzLczU_DFQ7lK90AxJKX47e-QTyOVE2FsTTkoQ,2037
 akride/_utils/progress_manager/progress_step.py,sha256=FU4Ni4Qmo6GdPdrYfl2JUOZG_D-U7j0EQv7BCn6Fdy4,1646
 akride/_utils/rest/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/_utils/rest/requests_session_manager.py,sha256=4c1-g0mkLo_MvMjjKDPlMbNajeQZe8IzDcfzzV6JmEk,528
 akride/_utils/rest/rest_client.py,sha256=MNA42NaJCeDR0A85fzJUgJqFon-LxBDEseVmdR6BJeM,3468
 akride/_utils/rest/utils.py,sha256=5w20_GIqHkgvD08l1zF3xMwefbo2eiJhMiSkg4qTIGs,194
 akride/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-akride/core/_log.py,sha256=LW7PWngv1SMCpzO2XgPXadiHYK1Tbs9Ji9M3eCGZ5Hc,1450
+akride/core/_log.py,sha256=r-ywYP4tuQBSy1OpYz_4hVDOPiOCiWarb_W5ADeytk4,1450
 akride/core/_pipeline_executor.py,sha256=CqIWR_qTVNP4FTl612mWwfVsdDmFbR1ASP0CPj4V43o,15237
 akride/core/constants.py,sha256=deN_w44brWZvvB_YAw6wQ3yfBalIL9s1IDHddIzF-wU,1859
-akride/core/enums.py,sha256=PJk-U8KSJSTriDmx3UthV_-Bo8YmVubLM2OiwWKojxk,2284
-akride/core/exceptions.py,sha256=LZDTJQ_rDEHxqaMqrkw7PoysQPuo9CCskHo1K5luThw,1500
-akride/core/types.py,sha256=cHcC21Ks51z1t4F7eEFa3mr080FXwLzz_kohYZuNRNw,9614
+akride/core/enums.py,sha256=oBbLtTjMNVJounpdWaSnTTBZpFeO6RAuJfDvWcmyrho,2309
+akride/core/exceptions.py,sha256=17Xmy_nxzhT8coMaID9ioRhQhOcaaoq9f7lvMoOilxo,1500
+akride/core/types.py,sha256=hyz3CWVVcPyGK47JJnUumpc6oMfPvZ0tPajdSGyu-vQ,9614
 akride/core/_entity_managers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/_entity_managers/catalog_manager.py,sha256=6JyeJF-metpdxNFSA8HigdCjEgilb9TdpNVt_U43rh8,18527
 akride/core/_entity_managers/dataset_manager.py,sha256=Qxu9lStk9K4l_RLqtHxFlavybKhKn4wODsj9ksZtISo,16073
-akride/core/_entity_managers/job_manager.py,sha256=44qGFvoKFDknom5DxDLkJLbu185Q6qODGoYG3gDFuoA,28206
+akride/core/_entity_managers/job_manager.py,sha256=OWrJtyL89ppXzcpQ8bCFNZWwytNrCNBpUv_6x0aoooM,30350
 akride/core/_entity_managers/manager.py,sha256=GxUe0oBhNbXyrrThs9S6j7AYF7qJI3FaXjXRbZl3MOo,2238
 akride/core/_entity_managers/resultset_manager.py,sha256=ToNDisHBP6Xb-L5OSEIPDlMvoiCSSErs8b6tV6t8VBI,8702
-akride/core/_entity_managers/subscriptions_manager.py,sha256=lPmC41MYl7kg_IBp84e9Lv3waqTokRJWhJZN6CclhQE,1314
+akride/core/_entity_managers/subscriptions_manager.py,sha256=q66WO0bmrxJhsHdgFd9dui1EaIxqtkU0a7cIfLRlZSc,1314
 akride/core/_filters/__init__.py,sha256=6CgtXVd_M0GUNJsbv9G939UenkUt91Nu28WDUq3_xwM,140
 akride/core/_filters/enums.py,sha256=NvVxgm3rkWJS0zMxbj7Cyxu2q3ExkYsUTfOedac7dQw,253
 akride/core/_filters/partitioners/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/_filters/partitioners/ingest_partitioner_filter.py,sha256=P5Gc_DZUexU5i4k71HRFMXZTyGAxAp7z8aaBW_3CVKg,6412
 akride/core/_filters/partitioners/models.py,sha256=ZIqpYx-4JUfAO60YTG_AFAa6ooFFjJ9N8OpSO098bv4,696
 akride/core/_filters/partitioners/partitioner_filter.py,sha256=fgzkq45fnSWO_ctJY9wxunad3FWcpN0OtvrCV7xaY2M,288
 akride/core/_filters/partitioners/process_partitioner_filter.py,sha256=6giTjKGInXAOTiMNJ_QJxTl1pno20XYAFtXACY2U2w4,6043
 akride/core/_filters/sink/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/_filters/sink/models.py,sha256=0YEzbZ-BpAI17yP3hA8i3yNHjVOvtAk5TXLqVLODcgY,1247
 akride/core/_filters/sink/sink_writer_filter.py,sha256=OHOzjNl5QMpDQFvyrF8fhKvqeTKC27YyjbkYdiJazRc,9178
 akride/core/conf/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/conf/pylogconf.yaml,sha256=fS9hD9BqxZePlDCISZUpma-7oKkFKbdhTFkm_BsbsRA,493
 akride/core/entities/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-akride/core/entities/catalogs.py,sha256=RPqPbjdG79yneZ85-BcrXtBGGwoGd2kUW8hyLGdQdRI,796
-akride/core/entities/datasets.py,sha256=N9VbHNuzqTOdlazECYwcNkpbY7zf_Mr_cn25m3ZtJS8,736
+akride/core/entities/catalogs.py,sha256=L1OcKZcL0EC3EOgAfJLIss7ptgJ3-QVDn5jvYgybmVM,796
+akride/core/entities/datasets.py,sha256=4mPUpPOJekjIrIpe_INEH6z9Yq-Q1VcXAkIeQHrIksE,736
 akride/core/entities/entity.py,sha256=sIRYQ69xM6Hrasn7edwgF-J6LjFG6hF7S95zTKe005g,1549
-akride/core/entities/jobs.py,sha256=7xsvcl6sjPEWWB_hUnq93nIwPVo9JI1rBrlhvxZnrDs,5133
+akride/core/entities/jobs.py,sha256=zaAiYcxmNkmF_ADuGk215K_W81YyO2uoZNQQTJUL844,5565
 akride/core/entities/pipeline.py,sha256=CF9hqAUIET75JhWc0nUm9oRDFIPf7lXOnMosbrIlfyw,436
-akride/core/entities/resultsets.py,sha256=Qlq5Ze1pAgi0jHFIVSrMps106leaxmGZgdHefXIfy50,955
+akride/core/entities/resultsets.py,sha256=6ZiJGB_MGOfrgoSQgMyreLhxXH60Ygz7scLohGTpue0,955
 akride/core/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 akride/core/models/catalog_details.py,sha256=JGDYdWM5EmF0f0vRHGaH6XtixqJkfH1b93Fgj3t3A1U,398
 akride/core/models/progress_info.py,sha256=5QBfsMe74CpYmmbnjYtr8CWsjhlhgb6yuwbS1srr_YQ,246
-akride-0.4.10.dist-info/LICENSE.txt,sha256=1FQFvIUl224RHI5tv0i_Ec4Edw9VKFiMUIoM3J6fmoA,130
-akride-0.4.10.dist-info/METADATA,sha256=IXnZit0jFS7MqBpGA30ikPLXHxKAVPMOkD3mBEP8-ds,3475
-akride-0.4.10.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-akride-0.4.10.dist-info/entry_points.txt,sha256=fD59ZhGa6sPkQd4H45zttMqxPXTHzJM7vQVJRLpGZns,44
-akride-0.4.10.dist-info/top_level.txt,sha256=qhymMhnBUjXghMz0FSS8l-t6VRF_MXcHLtW9nIP26c0,7
-akride-0.4.10.dist-info/RECORD,,
+akride-0.4.16.dist-info/LICENSE.txt,sha256=rCByYn1oXQOgnl6cQSmp1uiJlT4E0QM0PDchFzJimjo,130
+akride-0.4.16.dist-info/METADATA,sha256=bBQ3cJngbSIaR7iCszEJ2FczLrg8dE83epZRtA5RRlI,3490
+akride-0.4.16.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+akride-0.4.16.dist-info/entry_points.txt,sha256=fD59ZhGa6sPkQd4H45zttMqxPXTHzJM7vQVJRLpGZns,44
+akride-0.4.16.dist-info/top_level.txt,sha256=qhymMhnBUjXghMz0FSS8l-t6VRF_MXcHLtW9nIP26c0,7
+akride-0.4.16.dist-info/RECORD,,
```

